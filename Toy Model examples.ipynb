{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.stats as st \n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from patsy import dmatrices\n",
    "from sklearn.ensemble.forest import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "from pymc3 import Normal, Binomial, sample, Model # Import relevant distributions\n",
    "from pymc3.math import invlogit\n",
    "# Use a theano shared variable to be able to exchange the data the model runs on\n",
    "from theano import shared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from matplotlib import pyplot\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate Data (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TOY DATA LOGISTIC Regression\n",
    "#> set.seed(666)\n",
    "np.random.seed(seed=233423)\n",
    "x1 = st.norm.rvs(size=10000)           # some continuous variables \n",
    "x2 = st.norm.rvs(size=10000)  \n",
    "z = 1 + 2 *x1 +3 *x2        # linear combination with a bias\n",
    "pr = 1/(1 +np.exp(-z))         # pass through an inv-logit function\n",
    "y = st.binom.rvs(n=1,p=pr, size=10000) #rbinom(1000,1,pr) # bernoulli response variable\n",
    " \n",
    "X=np.column_stack([x1,x2])\n",
    "# standardize the features since regularization requires all features to be on same scale\n",
    "scaler = StandardScaler(copy=True)\n",
    "# we have created a standardization based on the training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaler.fit(X).transform(X), y, test_size=0.33, random_state=42)\n",
    "\n",
    "#now feed it to glm:\n",
    "#df = data.frame(y=y,x1=x1,x2=x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u.feature_importances_ [ 0.58108109  0.41891891]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.85223806817828562"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u=xgb.XGBClassifier()\n",
    "\n",
    "u.set_params( learning_rate= 0.1, max_depth= 4, n_estimators= 5 )\n",
    "u.fit(X_train, y_train)\n",
    "\n",
    "print('u.feature_importances_',u.feature_importances_)\n",
    "cross_val_score(u, X_train, y_train, cv=10 ).mean()\n",
    "\n",
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f0': 336.2849116279069, 'f1': 575.6274193548386}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u.booster().get_score(importance_type='cover')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importance type can be defined as:\n",
    "- 'weight' - the number of times a feature is used to split the data across all trees.\n",
    "- 'gain' - the average gain of the feature when it is used in trees\n",
    "- 'cover' - the average coverage of the feature when it is used in trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1e929dd23c8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEWCAYAAACOv5f1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1KD/DxbSMgVwKSpkAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e92415d8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgb.plot_importance(u,importance_type=\"weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cover Branch 1 1675.0\n",
      "Cover Branch 2 [ 694.] \n",
      "Cover Branch 3 [ 442.25]\n"
     ]
    }
   ],
   "source": [
    "#COVER\n",
    "p = u.predict_proba( X_train, ntree_limit=0)\n",
    "FF = pd.DataFrame(X_train)\n",
    "print('Cover Branch 1',len(FF)*0.5*0.5) #Cover\n",
    "#sum of the hessians in that node,(root node has all data)\n",
    "FF['y'] = y_train\n",
    "FF['Prob0'] = pd.DataFrame(p)[0]\n",
    "FF['Prob1'] = pd.DataFrame(p)[1]\n",
    "FF['Pstart'] = 0.5\n",
    "#next node\n",
    "print( 'Cover Branch 2',sum(np.array(FF[FF[1]<-0.225163][['Pstart']])*(1-np.array(FF[FF[1]<-0.225163][['Pstart']])))\n",
    ",'\\nCover Branch 3',sum(np.array(FF[(FF[1]<-0.225163) & (FF[0]<0.304833)][['Pstart']])*(1-np.array(FF[(FF[1]<-0.225163) & (FF[0]<0.304833)][['Pstart']]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method XGBModel.get_params of XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
       "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=4,\n",
       "       min_child_weight=1, missing=None, n_estimators=5, nthread=-1,\n",
       "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=1)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = u.booster().get_dump( with_stats=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:[f1<-0.225163] yes=1,no=2,missing=1,gain=1853.31,cover=1675\n",
      "\t1:[f0<0.304833] yes=3,no=4,missing=3,gain=486.058,cover=694\n",
      "\t\t3:[f1<-0.645459] yes=7,no=8,missing=7,gain=57.4649,cover=442.25\n"
     ]
    }
   ],
   "source": [
    "#First Tree\n",
    "for tree in [results[0]]:\n",
    "    for line in tree.split('\\n'):\n",
    "        #if  'leaf=' in line:\n",
    "        print(line)\n",
    "        #print(line)\n",
    "        if '3:[' in line:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1853.31\n"
     ]
    }
   ],
   "source": [
    "##### Gain for first tree\n",
    "z = 0.5 #Base score starts at .50\n",
    "nn= -0.225163\n",
    "p = np.repeat(z,len(FF))\n",
    "#2783\n",
    "# L = which(X[,'odor=none']==0) \n",
    "# R = which(X[,'odor=none']==1)\n",
    "\n",
    "pL = np.repeat(z,len(FF[FF[1]<nn]['y'])) #p[FF[1]<-0.324049]\n",
    "pR = np.repeat(z,len(FF[FF[1]>=nn]['y'])) #p[FF[1]>= -0.324049]\n",
    "\n",
    "yL = FF[FF[1]<nn]['y']\n",
    "yR = FF[FF[1]>=nn]['y']\n",
    "\n",
    "GL = sum(pL-yL)\n",
    "GR = sum(pR-yR)\n",
    "G = sum(p-np.array(FF['y']))\n",
    "#G = sum(np.array(FF['Prob1'])-np.array(FF['y']))\n",
    "\n",
    "HL = sum(pL*(1-pL))\n",
    "HR = sum(pR*(1-pR))\n",
    "H = sum(p*(1-p))\n",
    "\n",
    "gain = 0.5*((GL**2/(HL+1))+(GR**2/(HR+1))-(G**2/(H+1)))\n",
    "print(round(gain*2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>Prob0</th>\n",
       "      <th>Prob1</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.545257</td>\n",
       "      <td>-0.653484</td>\n",
       "      <td>0.495375</td>\n",
       "      <td>0.504625</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.332258</td>\n",
       "      <td>1.866386</td>\n",
       "      <td>0.451967</td>\n",
       "      <td>0.548033</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.227723</td>\n",
       "      <td>-0.269642</td>\n",
       "      <td>0.466756</td>\n",
       "      <td>0.533244</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.006776</td>\n",
       "      <td>-0.993703</td>\n",
       "      <td>0.538894</td>\n",
       "      <td>0.461106</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.202238</td>\n",
       "      <td>0.914339</td>\n",
       "      <td>0.463489</td>\n",
       "      <td>0.536511</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1     Prob0     Prob1  y\n",
       "0  0.545257 -0.653484  0.495375  0.504625  0\n",
       "1 -0.332258  1.866386  0.451967  0.548033  1\n",
       "2  1.227723 -0.269642  0.466756  0.533244  1\n",
       "3 -0.006776 -0.993703  0.538894  0.461106  0\n",
       "4 -1.202238  0.914339  0.463489  0.536511  1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Next tree\n",
    "tt = u.predict_proba(X_train,output_margin=False,ntree_limit=1)\n",
    "FF1 = pd.DataFrame(X_train)\n",
    "FF1['Prob0'] = pd.DataFrame(tt)[0]\n",
    "FF1['Prob1'] = pd.DataFrame(tt)[1]\n",
    "FF1['y'] =y_train\n",
    "FF1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:[f1<-0.225163] yes=1,no=2,missing=1,gain=1512.82,cover=1665.44\n",
      "\t1:[f0<0.304833] yes=3,no=4,missing=3,gain=397.685,cover=690.434\n",
      "\t\t3:[f1<-0.645459] yes=7,no=8,missing=7,gain=47.6899,cover=439.491\n"
     ]
    }
   ],
   "source": [
    "#Second Tree\n",
    "for tree in [results[1]]:\n",
    "    for line in tree.split('\\n'):\n",
    "        #if  'leaf=' in line:\n",
    "        print(line)\n",
    "        #print(line)\n",
    "        if '3:[' in line:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1512.82\n"
     ]
    }
   ],
   "source": [
    "########  GAIN of 2nd tree\n",
    "#Using prior trees probabilty Calculate the new G and H\n",
    "\n",
    "nn= -0.225163\n",
    "p = FF1['Prob1']\n",
    "\n",
    "pL = FF1[FF1[1]<nn]['Prob1']\n",
    "pR = FF1[FF1[1]>=nn]['Prob1']\n",
    "\n",
    "yL = FF1[FF1[1]<nn]['y']\n",
    "yR = FF1[FF1[1]>=nn]['y']\n",
    "\n",
    "GL = sum(pL-yL)\n",
    "GR = sum(pR-yR)\n",
    "G = sum(p-np.array(FF1['y']))\n",
    "\n",
    "HL = sum(pL*(1-pL))\n",
    "HR = sum(pR*(1-pR))\n",
    "H = sum(p*(1-p))\n",
    "\n",
    "gain = 0.5*((GL**2/(HL+1))+(GR**2/(HR+1))-(G**2/(H+1)))\n",
    "print(round(gain*2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###  XGBOOST RESOURCES\n",
    "\n",
    "#https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf\n",
    "#https://stackoverflow.com/questions/33520460/how-is-xgboost-cover-calculated/33614843\n",
    "# https://chaoticsenses.wordpress.com/2015/09/20/xgboost-a-macroscopic-anatomy/\n",
    "# http://www.grroups.com/blog/an-information-gain-based-feature-ranking-function-for-xgboost\n",
    "# https://machinelearningmastery.com/avoid-overfitting-by-early-stopping-with-xgboost-in-python/\n",
    "# https://cran.r-project.org/web/packages/xgboost/xgboost.pdf\n",
    "# https://machinelearningmastery.com/xgboost-python-mini-course/\n",
    "# https://machinelearningmastery.com/avoid-overfitting-by-early-stopping-with-xgboost-in-python/\n",
    "# https://stats.stackexchange.com/questions/231220/\n",
    "# how-to-compute-the-gradient-and-hessian-of-logarithmic-loss-question-is-based/231270\n",
    "# https://arxiv.org/pdf/1603.02754.pdf\n",
    "# https://arxiv.org/pdf/1606.05390.pdf\n",
    "# https://jessesw.com/XG-Boost/\n",
    "# https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "# https://datascience.stackexchange.com/questions/12318/how-do-i-interpret-the-output-of-xgboost-importance\n",
    "# https://stackoverflow.com/questions/33520460/how-is-xgboost-cover-calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f0': 43, 'f1': 31} {'f0': 98.5243472883721, 'f1': 286.6727258064515} gain=\n"
     ]
    }
   ],
   "source": [
    "#### Calculate GAIN or COVER for the entire Model (ALL TREES)\n",
    "importance_type = 'gain' #'cover'\n",
    "importance_type += '='\n",
    "fmap = {}\n",
    "gmap = {}\n",
    "for tree in results:\n",
    "    for line in tree.split('\\n'):\n",
    "        # look for the opening square bracket\n",
    "        arr = line.split('[')\n",
    "        #print('arr: ',arr, len(arr))\n",
    "        # if no opening bracket (leaf node), ignore this line\n",
    "        if len(arr) == 1:\n",
    "            #print('arr: ',arr, len(arr))\n",
    "            continue\n",
    "\n",
    "        # look for the closing bracket, extract only info within that bracket\n",
    "        fid = arr[1].split(']')\n",
    "        #print('fid] : ',fid)\n",
    "        # extract gain or cover from string after closing bracket\n",
    "        g = float(fid[1].split(importance_type)[1].split(',')[0])\n",
    "        #print('g: ',g)\n",
    "        # extract feature name from string before closing bracket\n",
    "        fid = fid[0].split('<')[0]\n",
    "        #print('fid< : ',fid)\n",
    "\n",
    "        if fid not in fmap:\n",
    "            # if the feature hasn't been seen yet\n",
    "            fmap[fid] = 1\n",
    "            gmap[fid] = g\n",
    "        else:\n",
    "            fmap[fid] += 1\n",
    "            gmap[fid] += g\n",
    "\n",
    "# calculate average value (gain/cover) for each feature\n",
    "for fid in gmap:\n",
    "    gmap[fid] = gmap[fid] / fmap[fid]\n",
    "\n",
    "print(fmap,gmap,importance_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight {'f0': 9, 'f1': 6}\n"
     ]
    }
   ],
   "source": [
    "######  Calculate WEIGHT for the entire Model (ALL TREES)\n",
    "fmap={}\n",
    "for tree in [results[0]]:\n",
    "    for line in tree.split('\\n'):\n",
    "        # look for the opening square bracket\n",
    "        arr = line.split('[')\n",
    "        #print('arr :',arr,\"len(arr) == 1 : \",len(arr) == 1,len(arr))\n",
    "                    # if no opening bracket (leaf node), ignore this line\n",
    "        if len(arr) == 1:\n",
    "            #print(len(arr) == 1,'arr :',arr)\n",
    "            continue\n",
    "        # extract feature name from string between []\n",
    "        fid = arr[1].split(']')[0].split('<')[0]\n",
    "        #print('fid :',fid)\n",
    "        if fid not in fmap:\n",
    "        # if the feature hasn't been seen yet\n",
    "            fmap[fid] = 1\n",
    "        else:\n",
    "            fmap[fid] += 1\n",
    "print('Weight',fmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SplitValue</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.249088</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.141577</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.034065</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.926553</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.711530</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.604018</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.496506</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.388994</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.281483</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.173971</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.148564</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.471100</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.578611</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.116170</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SplitValue  Count\n",
       "0    -1.249088    2.0\n",
       "1    -1.141577    1.0\n",
       "2    -1.034065    1.0\n",
       "3    -0.926553    1.0\n",
       "4    -0.711530    4.0\n",
       "5    -0.604018    2.0\n",
       "6    -0.496506    1.0\n",
       "7    -0.388994    2.0\n",
       "8    -0.281483    1.0\n",
       "9    -0.173971    2.0\n",
       "10    0.148564    5.0\n",
       "11    0.471100    4.0\n",
       "12    0.578611    4.0\n",
       "13    1.116170    1.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u.booster().get_split_value_histogram('f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.581081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.418919</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    weights\n",
       "0  0.581081\n",
       "1  0.418919"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(u.feature_importances_, columns=['weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1e92a261cc0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQwAAAD8CAYAAACCaZo+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e92a3b30f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgb.plot_tree(u, num_trees=4, rankdir='LR') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def order_features_by_gains(bst, feature_map_file=None):\n",
    "    str_dump = bst.booster().get_dump(with_stats=True)\n",
    "    \n",
    "    tree_arr = []\n",
    "    for i_tree, tree in enumerate(str_dump):\n",
    "        arr_lvls=tree.split('\\n\\t')\n",
    "        a_tree = {}\n",
    "        for lvl in arr_lvls:\n",
    "            a_lvl ={}\n",
    "            dum1 = lvl.split(',')\n",
    "            if('leaf' in lvl):\n",
    "                dum1[0].replace('\\t','')\n",
    "                dum10 = dum1[0].split(':')\n",
    "                lvl_id = int(dum10[0])\n",
    "                dum11 = dum10[1].split('leaf=')\n",
    "                leaf = float(dum11[1])\n",
    "                \n",
    "                cover = float(dum1[1].replace('\\n','').split('cover=')[1])\n",
    "                a_lvl['lvl_id']=lvl_id\n",
    "                a_lvl['leaf']=leaf\n",
    "                a_lvl['cover']=cover\n",
    "            else:\n",
    "                dum10 = dum1[0].replace('\\t','').replace('\\n','')\n",
    "                dum11 = dum10.split(':')\n",
    "                lvl_id = int(dum11[0])\n",
    "                dum12 = dum11[1].split('yes=')\n",
    "                dum13 = dum12[0].replace('[','').replace(']','').split('<')\n",
    "                feat_name = dum13[0]\n",
    "                \n",
    "                yes_to = int(dum12[1])\n",
    "                no_to = int(dum1[1].split('no=')[1])\n",
    "                missing = int(dum1[2].split('missing=')[1])\n",
    "                gain = float(dum1[3].split('gain=')[1])\n",
    "                cover = float(dum1[4].split('cover=')[1])            \n",
    "                feat_thr = float(dum12[1])\n",
    "                \n",
    "                a_lvl['lvl_id']=lvl_id\n",
    "                a_lvl['feat_name']=feat_name\n",
    "                a_lvl['feat_thr'] = feat_thr\n",
    "                a_lvl['yes_to'] = yes_to\n",
    "                a_lvl['no_to']=no_to\n",
    "                a_lvl['missing'] = missing\n",
    "                a_lvl['gain']=gain\n",
    "                a_lvl['cover']=cover\n",
    "                \n",
    "            a_tree[str(lvl_id)] = a_lvl\n",
    "        tree_arr.append(a_tree)    \n",
    "    feat_vocabulary = {}\n",
    "    for tree in tree_arr:\n",
    "        for lvl in tree:\n",
    "            if('gain' in tree[lvl]):\n",
    "                feat_data = feat_vocabulary.setdefault(tree[lvl]['feat_name'],\\\n",
    "                                                       {'gain':tree[lvl]['gain'],'cover':tree[lvl]['cover']})\n",
    "                if(feat_data!={'gain':tree[lvl]['gain'],'cover':tree[lvl]['cover']}):\n",
    "                    try:\n",
    "                        feat_vocabulary[tree[lvl]['feat_name']]['gain'] += tree[lvl]['gain']                    \n",
    "                        feat_vocabulary[tree[lvl]['feat_name']]['cover'] += tree[lvl]['cover']\n",
    "                    except:\n",
    "                        feat_vocabulary[tree[lvl]['feat_name']]['gain'] = tree[lvl]['gain']                    \n",
    "                        feat_vocabulary[tree[lvl]['feat_name']]['cover'] = tree[lvl]['cover']          \n",
    "    \n",
    "    sorted_feats = sorted(feat_vocabulary.items(),key=lambda k:k[1]['gain'], reverse=True)\n",
    "    return sorted_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('f1', {'cover': 17844.45, 'gain': 8886.8545}),\n",
       " ('f0', {'cover': 14460.251200000002, 'gain': 4236.546933399999})]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_features_by_gains(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate Data (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TOY DATA LOGISTIC Regression\n",
    "\n",
    "np.random.seed(seed=233423)\n",
    "x1 = st.norm.rvs(size=10000)           # some continuous variables \n",
    "x2 = st.norm.rvs(size=10000)  \n",
    "z = 1 + 2 *x1 +3 *x2        # linear combination with a bias\n",
    "pr = 1/(1 +np.exp(-z))         # pass through an inv-logit function\n",
    "y = st.binom.rvs(n=1,p=pr, size=10000) #rbinom(1000,1,pr) # bernoulli response variable\n",
    " \n",
    "X=np.column_stack([x1,x2])\n",
    "# standardize the features since regularization requires all features to be on same scale\n",
    "scaler = StandardScaler(copy=True)\n",
    "# we have created a standardization based on the training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaler.fit(X).transform(X), y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model=xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate=[0.0001,0.001,0.01,0.1,0.2,0.3]\n",
    "n_estimators=[50,100,150,200]\n",
    "max_depth=[2,4,6,8]\n",
    "param_grid=dict(learning_rate=learning_rate,max_depth=max_depth,n_estimators=n_estimators)\n",
    "kfold=StratifiedKFold(n_splits=10,shuffle=True,random_state=7)\n",
    "grid_search=GridSearchCV(model,param_grid,n_jobs=-1,cv=kfold)\n",
    "#grid_result=grid_search.fit(X,y)\n",
    "grid_result=grid_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.85452104,  0.85982983,  0.85490372])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Take forever\n",
    "cross_val_score(grid_result, X_train, y_train, cv=3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistic Regression Bayesian Inference approach :\n",
    "    - Pymc3 with Theano\n",
    "\n",
    "    Steps:\n",
    "        1. Prepare Data\n",
    "        2. Build Probabilistic Model\n",
    "        3. Condition Model on Data & Find the local maximum a posteriori point given a model (MAP)\n",
    "        4. Sample posterior distribution using MAP as starting points for Indepedent Variables(X's)\n",
    "        5. Generate posterior predictive samples from model given a Samples of posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "logp = -2,093.8, ||grad|| = 0.67802: 100%|█████████████████████████████████████████████| 14/14 [00:00<00:00, 22.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'beta': array(2.948381992072162), 'beta0': array(1.9832358477639138), 'alpha': array(0.9321311638813541)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "logp = -2,093.8, ||grad|| = 0.67802: 100%|█████████████████████████████████████████████| 14/14 [00:00<00:00, 23.41it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [04:01<00:00,  8.28it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1011.03it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2893.07it/s]\n"
     ]
    }
   ],
   "source": [
    "# 1 Use theano shared variable so that we can make predictions for new values later\n",
    "log_dose_shared0 = shared(X_train[:, 0])\n",
    "log_dose_shared = shared(X_train[:, 1])\n",
    "\n",
    "# Sample size in each group. The sample size has to be a shared variable too\n",
    "# Each row/observation is a group so n = total in group. 1 if only one per group\n",
    "n_shared = shared(np.ones(len(X_train), dtype=int))\n",
    "\n",
    "# Outcomes/Target\n",
    "deaths = y_train\n",
    "\n",
    "\n",
    "# 2 Build Probabilistic Model\n",
    "with Model() as bioassay_model:\n",
    "\n",
    "    # Priors for unknown model parameters. e.g. Logit-linear model parameters\n",
    "    alpha = Normal('alpha', 0, sd=100)\n",
    "    beta = Normal('beta', 0, sd=100)\n",
    "    beta0 = Normal('beta0', 0, sd=100)\n",
    "\n",
    "    # Expected value of outcome. e.g. link function outcome. Calculate probabilities of Y/Target\n",
    "    theta = invlogit(alpha + beta * log_dose_shared + beta0 * log_dose_shared0 )\n",
    "\n",
    "    # Likelihood (sampling distribution) of observations Data likelihood YTarget\n",
    "    obs_deaths = Binomial('obs_deaths', n=n_shared, p=theta, observed=deaths)\n",
    "\n",
    "    \n",
    "# 3 Finds the local maximum a posteriori point given a model. uses BFGS.\n",
    "from pymc3 import find_MAP\n",
    "# Runs fit to data returns parameters/coefficients\n",
    "map_estimate = find_MAP(model=bioassay_model)\n",
    "print(map_estimate)\n",
    "\n",
    "\n",
    "# 4 Now draw samples from the posterior using the given step methods.\n",
    "with bioassay_model:\n",
    "    \n",
    "    # obtain starting values via MAP\n",
    "    start = find_MAP(model=bioassay_model)\n",
    "    \n",
    "    # instantiate sampler\n",
    "    step = pm.Metropolis()\n",
    "    \n",
    "    # posterior of X's\n",
    "    # draw 1,000 posterior samples of independent variables\n",
    "    bioassay_trace = sample(1500, step=step, start=start)\n",
    "\n",
    "\n",
    "# 5 Generate posterior predictive samples from a model given a trace.\n",
    "from pymc3 import sample_ppc\n",
    "\n",
    "with bioassay_model:\n",
    "    deaths_sim = sample_ppc(bioassay_trace, samples=1000)\n",
    "    \n",
    "# take only last half  of posterior distr. of X's. other half was burn in.\n",
    "tr1 = bioassay_trace[500:]\n",
    "    \n",
    "#PREDICT\n",
    "log_dose_to_predict0 = X_train[:1000,0] #np.random.uniform(-0.8,0.7,size=50)\n",
    "log_dose_to_predict = X_train[:1000,1] #np.random.uniform(-0.8,0.7,size=50)\n",
    "n_predict = n = np.ones(1000, dtype=int)\n",
    "\n",
    "# Changing values here will also change values in the model\n",
    "log_dose_shared0.set_value(log_dose_to_predict0)\n",
    "log_dose_shared.set_value(log_dose_to_predict)\n",
    "n_shared.set_value(n_predict)\n",
    "\n",
    "# Simply running PPC will use the updated values and do prediction\n",
    "ppc = pm.sample_ppc(tr1, model=bioassay_model, samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.540698\n"
     ]
    }
   ],
   "source": [
    "print( 'Accuracy:',(ppc['obs_deaths']==y[:1000]).mean() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Changing values here will also change values in the model\n",
    "log_dose_shared0.set_value(np.array([ 0.93622719]))\n",
    "log_dose_shared.set_value(np.array([-1.2161206]))\n",
    "n_shared.set_value(np.array([1]))\n",
    "\n",
    "# Simply running PPC will use the updated values and do prediction\n",
    "ppc = pm.sample_ppc(tr1, model=bioassay_model, samples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logitInv= lambda x: np.exp(x)/(1.0+np.exp(x)) #sigmoid --> returns probability\n",
    "logitInv(0.9053831851570006 + 2.9168537609096776 * 0.08986007 + 1.9854704735996134 * 2.57440271)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.39994053  0.60005947]\n",
      "0.828507606654\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(max_depth=100,max_features='sqrt', random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.feature_importances_)\n",
    "\n",
    "print( cross_val_score(clf, X_train, y_train, cv=10 ).mean() ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.1,  0.9]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(clf.predict([[2.57440271,  0.08986007]]))\n",
    "clf.predict_proba([[2.57440271,  0.08986007]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression - Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.96671609  2.9239234 ]]\n",
      "0.856566652147\n"
     ]
    }
   ],
   "source": [
    "#logistic Regression\n",
    "logit = LogisticRegression(fit_intercept=True)\n",
    "\n",
    "# Fit model. Let X_train = matrix of predictors, y_train = matrix of variable.\n",
    "# NOTE: Do not include a column for the intercept when fitting the model.\n",
    "resLogit = logit.fit(X_train, y_train)\n",
    "print(resLogit.coef_)\n",
    "print(cross_val_score(resLogit, X_train, y_train, cv=10 ).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.852238068178 0.828507606654 0.856566652147\n"
     ]
    }
   ],
   "source": [
    "print( cross_val_score(u, X_train, y_train, cv=10 ).mean() ,\n",
    "cross_val_score(clf, X_train, y_train, cv=10 ).mean(),\n",
    "cross_val_score(resLogit, X_train, y_train, cv=10 ).mean() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e92c418f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "probs = u.predict_proba(X_train)\n",
    "preds = probs[:,1]\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_train, preds)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "pforest = clf.predict_proba(X_train)\n",
    "pforest1 = pforest[:,1]\n",
    "fpr1, tpr1, threshold1 = metrics.roc_curve(y_train, pforest1)\n",
    "roc_auc1 = metrics.auc(fpr1, tpr1)\n",
    "\n",
    "plog = logit.predict_proba(X_train)\n",
    "plog1 = plog[:,1]\n",
    "fpr2, tpr2, threshold2 = metrics.roc_curve(y_train, plog1)\n",
    "roc_auc2 = metrics.auc(fpr2, tpr2)\n",
    "\n",
    "# method I: plt\n",
    "#import matplotlib.pyplot as plt\n",
    "pyplot.title('Receiver Operating Characteristic')\n",
    "pyplot.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "pyplot.plot(fpr1, tpr1, 'g', label = 'AUC_f = %0.2f' % roc_auc1)\n",
    "pyplot.plot(fpr2, tpr2, 'y', label = 'AUC_L = %0.2f' % roc_auc2)\n",
    "pyplot.legend(loc = 'lower right')\n",
    "pyplot.plot([0, 1], [0, 1],'r--')\n",
    "pyplot.xlim([0, 1])\n",
    "pyplot.ylim([0, 1])\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate Data (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Toy Data Random Forest data\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=4,\n",
    "                            n_informative=2, n_redundant=0,\n",
    "                            random_state=0, shuffle=False)\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaler.fit(X).transform(X), y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(max_depth=100,max_features='sqrt', random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(clf.feature_importances_)\n",
    "\n",
    "print(cross_val_score(clf, X_train, y_train, cv=10 ),\n",
    "np.abs(cross_val_score(clf, X_train, y_train, cv=10 ).mean()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(clf.predict([[-0.57643759, -0.6723759 , -0.23639363,  0.54680607]]) , \n",
    "      clf.predict_proba([[-0.57643759, -0.6723759 , -0.23639363,  0.54680607]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression - Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeff:  [[ 1.96671609  2.9239234 ]]\n",
      "Acc:  [ 0.85245902  0.84925373  0.85820896  0.85522388  0.85373134  0.8641791\n",
      "  0.86268657  0.8641791   0.85671642  0.8490284 ] 0.856566652147\n"
     ]
    }
   ],
   "source": [
    "logit = LogisticRegression(fit_intercept=True)\n",
    "\n",
    "# Fit model. Let X_train = matrix of predictors, y_train = matrix of variable.\n",
    "# NOTE: Do not include a column for the intercept when fitting the model.\n",
    "resLogit = logit.fit(X_train, y_train)\n",
    "print('Coeff: ',resLogit.coef_)\n",
    "print('Acc: ',cross_val_score(resLogit, X_train, y_train, cv=10 )\n",
    "      , np.abs(cross_val_score(resLogit, X_train, y_train, cv=10 ).mean()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Sklearn vs Statsmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take aways:\n",
    "- Standard Error is the square root of the covariance matrix. Sklearn doesn't provide it.\n",
    "- Increase C in Sklearn to match Statsmodel output. Statsmodel isn't regularized and can't turn off Regularizer in Sklearn. It defaults to L2 and a high C negates regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard errors:  [ 0.04346896  0.05998815  0.07749697]\n",
      "Coefficients:     [ 0.93213188  1.98323743  2.94838434]\n"
     ]
    }
   ],
   "source": [
    "# Initiate logistic regression object\n",
    "logit = LogisticRegression(C=1e9,fit_intercept=True)\n",
    "\n",
    "# Fit model. Let X_train = matrix of predictors, y_train = matrix of variable.\n",
    "# NOTE: Do not include a column for the intercept when fitting the model.\n",
    "resLogit = logit.fit(X_train, y_train)\n",
    "#print(resLogit.intercept_,resLogit.coef_)\n",
    "\n",
    "# Calculate matrix of predicted class probabilities. \n",
    "# Check resLogit.classes_ to make sure that sklearn ordered your classes as expected\n",
    "predProbs = np.matrix(resLogit.predict_proba(X_train))\n",
    "\n",
    "# Design matrix -- add column of 1's at the beginning of your X_train matrix\n",
    "X_design = np.column_stack((np.ones(shape = X_train.shape[0]), X_train))\n",
    "#np.ones(shape = X_train.shape[0])\n",
    "#X_design =X_train\n",
    "\n",
    "# Initiate matrix of 0's, fill diagonal with each predicted observation's variance\n",
    "V = np.matrix(np.zeros(shape = (X_design.shape[0], X_design.shape[0])))\n",
    "np.fill_diagonal(V, np.multiply(predProbs[:,0], predProbs[:,1]).A1)\n",
    "\n",
    "# Covariance matrix\n",
    "covLogit = np.linalg.inv(X_design.T * V * X_design)\n",
    "#print(\"Covariance matrix: \", covLogit)\n",
    "\n",
    "# Standard errors\n",
    "print(\"Standard errors: \", np.sqrt(np.diag(covLogit)) )\n",
    "\n",
    "# Wald statistic (coefficient / s.e.) ^ 2\n",
    "logitParams = np.insert(resLogit.coef_, 0, resLogit.intercept_)\n",
    "print(\"Coefficients:    \",logitParams)\n",
    "#print( \"Wald statistics: \", (logitParams / np.sqrt(np.diag(covLogit))) ** 2)\n",
    "\n",
    "# Sklearn vs StatsModel Reference:\n",
    "# https://stackoverflow.com/questions/24924755/logit-estimator-in-statsmodels-and-sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.025</th>\n",
       "      <th>Coef.</th>\n",
       "      <th>0.975</th>\n",
       "      <th>St.Errors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Const.</th>\n",
       "      <td>0.847</td>\n",
       "      <td>0.932</td>\n",
       "      <td>1.017</td>\n",
       "      <td>0.043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x1</th>\n",
       "      <td>1.866</td>\n",
       "      <td>1.983</td>\n",
       "      <td>2.101</td>\n",
       "      <td>0.060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x2</th>\n",
       "      <td>2.796</td>\n",
       "      <td>2.948</td>\n",
       "      <td>3.100</td>\n",
       "      <td>0.077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0.025  Coef.  0.975  St.Errors\n",
       "Const.  0.847  0.932  1.017      0.043\n",
       "x1      1.866  1.983  2.101      0.060\n",
       "x2      2.796  2.948  3.100      0.077"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SKLEARN\n",
    "#print(\"Standard errors: \", np.sqrt(np.diag(covLogit)) )\n",
    "logitParams = np.insert(resLogit.coef_, 0, resLogit.intercept_)\n",
    "Intervals = pd.DataFrame({'0.975':[round(float(c+(1.96*v)),3) for c,v in zip(logitParams,np.sqrt(np.diag(covLogit)))],\n",
    "             'Coef.':[round(float(x),3) for x in logitParams],\n",
    "             '0.025':[round(float(c-(1.96*v)),3) for c,v in zip(logitParams,np.sqrt(np.diag(covLogit)))]}\n",
    "             ,index=['Const.','x1','x2'])\n",
    "Intervals['St.Errors']=[round(float(c),3) for c in np.sqrt(np.diag(covLogit))]\n",
    "Intervals[['0.025', 'Coef.', '0.975','St.Errors']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Logistic Regression - statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.310040\n",
      "         Iterations 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>y</td>        <th>  No. Observations:  </th>  <td>  6700</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>  <td>  6697</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>  <td>     2</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Tue, 14 Nov 2017</td> <th>  Pseudo R-squ.:     </th>  <td>0.5409</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>01:49:23</td>     <th>  Log-Likelihood:    </th> <td> -2077.3</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th> <td> -4524.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th>  <td> 0.000</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    0.9321</td> <td>    0.043</td> <td>   21.444</td> <td> 0.000</td> <td>    0.847</td> <td>    1.017</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    1.9832</td> <td>    0.060</td> <td>   33.060</td> <td> 0.000</td> <td>    1.866</td> <td>    2.101</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    2.9484</td> <td>    0.077</td> <td>   38.045</td> <td> 0.000</td> <td>    2.796</td> <td>    3.100</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   No. Observations:                 6700\n",
       "Model:                          Logit   Df Residuals:                     6697\n",
       "Method:                           MLE   Df Model:                            2\n",
       "Date:                Tue, 14 Nov 2017   Pseudo R-squ.:                  0.5409\n",
       "Time:                        01:49:23   Log-Likelihood:                -2077.3\n",
       "converged:                       True   LL-Null:                       -4524.5\n",
       "                                        LLR p-value:                     0.000\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          0.9321      0.043     21.444      0.000       0.847       1.017\n",
       "x1             1.9832      0.060     33.060      0.000       1.866       2.101\n",
       "x2             2.9484      0.077     38.045      0.000       2.796       3.100\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.formula.api as sm\n",
    "model = sm.Logit(y_train, X_design)\n",
    "result =model.fit() #model.fit(method='bfgs')\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistic Regression Bayesian Inference approach :\n",
    "    - Pymc3 with Theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1 Use theano shared variable so that we can make predictions for new values later\n",
    "log_dose_shared0 = shared(X_train[:, 0])\n",
    "log_dose_shared1 = shared(X_train[:, 1])\n",
    "log_dose_shared2 = shared(X_train[:, 2])\n",
    "log_dose_shared3 = shared(X_train[:, 3])\n",
    "\n",
    "# Sample size in each group. The sample size has to be a shared variable too\n",
    "# Each row/observation is a group so n = total in group. 1 if only one per group\n",
    "n_shared = shared(np.ones(len(X_train), dtype=int))\n",
    "\n",
    "# Outcomes/Target\n",
    "deaths = y_train\n",
    "\n",
    "\n",
    "# 2 Build Probabilistic Model\n",
    "with Model() as bioassay_model:\n",
    "\n",
    "    # Priors for unknown model parameters. e.g. Logit-linear model parameters\n",
    "    alpha = Normal('alpha', 0, sd=100)\n",
    "    beta0 = Normal('beta0', 0, sd=100)\n",
    "    beta1 = Normal('beta1', 0, sd=100)\n",
    "    beta2 = Normal('beta2', 0, sd=100)\n",
    "    beta3 = Normal('beta3', 0, sd=100)\n",
    "    \n",
    "    # Expected value of outcome. e.g. link function outcome. Calculate probabilities of Y/Target\n",
    "    theta = invlogit(alpha + beta0 * log_dose_shared0 + beta1 * log_dose_shared1\\\n",
    "                     + beta2 * log_dose_shared2 + beta3 * log_dose_shared3 )\n",
    "\n",
    "    # Likelihood (sampling distribution) of observations Data likelihood YTarget\n",
    "    obs_deaths = Binomial('obs_deaths', n=n_shared, p=theta, observed=deaths)\n",
    "\n",
    "    \n",
    "# 3 Finds the local maximum a posteriori point given a model. uses BFGS.\n",
    "from pymc3 import find_MAP\n",
    "# Runs fit to data returns parameters/coefficients\n",
    "map_estimate = find_MAP(model=bioassay_model)\n",
    "print(map_estimate)\n",
    "\n",
    "\n",
    "# 4 Now draw samples from the posterior using the given step methods.\n",
    "with bioassay_model:\n",
    "    \n",
    "    # obtain starting values via MAP\n",
    "    start = find_MAP(model=bioassay_model)\n",
    "    \n",
    "    # instantiate sampler\n",
    "    step = pm.Metropolis()\n",
    "    \n",
    "    # posterior of X's\n",
    "    # draw 1,000 posterior samples of independent variables\n",
    "    bioassay_trace = sample(1000, step=step, start=start)\n",
    "\n",
    "\n",
    "# 5 Generate posterior predictive samples from a model given a trace.\n",
    "from pymc3 import sample_ppc\n",
    "\n",
    "with bioassay_model:\n",
    "    deaths_sim = sample_ppc(bioassay_trace, samples=1000)\n",
    "    \n",
    "# take only last half  of posterior distr. of X's. other half was burn in.\n",
    "tr1 = bioassay_trace[500:]\n",
    "    \n",
    "#PREDICT\n",
    "log_dose_to_predict0 = X_train[:1000,0] #np.random.uniform(-0.8,0.7,size=50)\n",
    "log_dose_to_predict1 = X_train[:1000,1] #np.random.uniform(-0.8,0.7,size=50)\n",
    "log_dose_to_predict2 = X_train[:1000,2] #np.random.uniform(-0.8,0.7,size=50)\n",
    "log_dose_to_predict3 = X_train[:1000,3] #np.random.uniform(-0.8,0.7,size=50)\n",
    "n_predict = n = np.ones(1000, dtype=int)\n",
    "\n",
    "# Changing values here will also change values in the model\n",
    "log_dose_shared0.set_value(log_dose_to_predict0)\n",
    "log_dose_shared1.set_value(log_dose_to_predict1)\n",
    "log_dose_shared2.set_value(log_dose_to_predict2)\n",
    "log_dose_shared3.set_value(log_dose_to_predict3)\n",
    "n_shared.set_value(n_predict)\n",
    "\n",
    "# Simply running PPC will use the updated values and do prediction\n",
    "ppc = pm.sample_ppc(tr1, model=bioassay_model, samples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print( 'Accuracy:',(ppc['obs_deaths']==y[:1000]).mean() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_train[249:251],y_train[249:251]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Changing values here will also change values in the model\n",
    "log_dose_shared0.set_value(np.array([-0.57643759]))\n",
    "log_dose_shared1.set_value(np.array([-0.6723759]))\n",
    "log_dose_shared2.set_value(np.array([-0.23639363]))\n",
    "log_dose_shared3.set_value(np.array([0.54680607]))\n",
    "n_shared.set_value(np.array([1]))\n",
    "\n",
    "# Simply running PPC will use the updated values and do prediction\n",
    "ppc = pm.sample_ppc(tr1, model=bioassay_model, samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ppc['obs_deaths'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Changing values here will also change values in the model\n",
    "log_dose_shared0.set_value(np.array([-0.98566996]))\n",
    "log_dose_shared1.set_value(np.array([1.12344181]))\n",
    "log_dose_shared2.set_value(np.array([-0.35003196]))\n",
    "log_dose_shared3.set_value(np.array([-1.1158904]))\n",
    "n_shared.set_value(np.array([1]))\n",
    "\n",
    "# Simply running PPC will use the updated values and do prediction\n",
    "ppc = pm.sample_ppc(tr1, model=bioassay_model, samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ppc['obs_deaths'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "map_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logitInv= lambda x: np.exp(x)/(1.0+np.exp(x)) #sigmoid --> returns probability\n",
    "logitInv(map_estimate['alpha']+map_estimate['beta0']*X_train[249:251][1][0]+\\\n",
    "         map_estimate['beta1']*X_train[249:251][1][1]\\\n",
    "+map_estimate['beta2']*X_train[249:251][1][2]+map_estimate['beta3']*X_train[249:251][1][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import random, math\n",
    "\n",
    "# def k_fold(data, myseed=11109, k=10):\n",
    "#     # Load data\n",
    "#     #data = open(myfile).readlines()\n",
    "\n",
    "#     # Shuffle input\n",
    "#     random.seed=myseed\n",
    "#     random.shuffle(data)\n",
    "\n",
    "#     # Compute partition size given input k\n",
    "#     len_part=int(math.ceil(len(data)/float(k)))\n",
    "\n",
    "#     # Create one partition per fold\n",
    "#     train={}\n",
    "#     test={}\n",
    "#     for ii in range(k):\n",
    "#         test[ii]  = data[ii*len_part:ii*len_part+len_part]\n",
    "#         train[ii] = [jj for jj in data if jj not in test[ii]]\n",
    "\n",
    "#     return train, test \n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# plt.errorbar(x=log_dose_to_predict0[:50], \\\n",
    "# y=np.asarray(ppc['obs_deaths']).mean(axis=0)[:50],\\\n",
    "# yerr=np.asarray(ppc['obs_deaths']).std(axis=0)[:50], linestyle='', marker='o')\n",
    "# plt.plot(X_train[:50, 1], deaths[:50], 'o')\n",
    "# plt.xlabel('log_dose',size=15)s\n",
    "# plt.ylabel('number of rats with tumors',size=15)\n",
    "\n",
    "#now feed it to glm:\n",
    "#df = data.frame(y=y,x1=x1,x2=x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rf = RandomForestRegressor(n_estimators = 100, max_features='sqrt')\n",
    "# rf.fit(X, y)\n",
    "# # feature importances\n",
    "# # the higher, the more important the feature\n",
    "# d = {'importance': rf.feature_importances_}\n",
    "# pd.DataFrame(d, index=X.columns).sort('importance')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
