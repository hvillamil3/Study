{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.stats as st \n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from patsy import dmatrices\n",
    "from sklearn.ensemble.forest import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "from pymc3 import Normal, Binomial, sample, Model # Import relevant distributions\n",
    "from pymc3.math import invlogit\n",
    "# Use a theano shared variable to be able to exchange the data the model runs on\n",
    "from theano import shared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hvill\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib import pyplot\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate Data (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TOY DATA LOGISTIC Regression\n",
    "#> set.seed(666)\n",
    "np.random.seed(seed=233423)\n",
    "x1 = st.norm.rvs(size=10000)           # some continuous variables \n",
    "x2 = st.norm.rvs(size=10000)  \n",
    "z = 1 + 2 *x1 +3 *x2        # linear combination with a bias\n",
    "pr = 1/(1 +np.exp(-z))         # pass through an inv-logit function\n",
    "y = st.binom.rvs(n=1,p=pr, size=10000) #rbinom(1000,1,pr) # bernoulli response variable\n",
    " \n",
    "X=np.column_stack([x1,x2])\n",
    "# standardize the features since regularization requires all features to be on same scale\n",
    "scaler = StandardScaler(copy=True)\n",
    "# we have created a standardization based on the training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaler.fit(X).transform(X), y, test_size=0.33, random_state=42)\n",
    "\n",
    "#now feed it to glm:\n",
    "#df = data.frame(y=y,x1=x1,x2=x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u.feature_importances_ [ 0.58108109  0.41891891]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.85223806817828562"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u=xgb.XGBClassifier()\n",
    "\n",
    "u.set_params( learning_rate= 0.1, max_depth= 4, n_estimators= 5 )\n",
    "u.fit(X_train, y_train)\n",
    "\n",
    "print('u.feature_importances_',u.feature_importances_)\n",
    "cross_val_score(u, X_train, y_train, cv=10 ).mean()\n",
    "\n",
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f0': 336.2849116279069, 'f1': 575.6274193548386}"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u.booster().get_score(importance_type='cover')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importance type can be defined as:\n",
    "- 'weight' - the number of times a feature is used to split the data across all trees.\n",
    "- 'gain' - the average gain of the feature when it is used in trees\n",
    "- 'cover' - the average coverage of the feature when it is used in trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1cd010d2fd0>"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEWCAYAAACOv5f1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1KD/DxbSMgVwKSpkAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cd01316ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgb.plot_importance(u,importance_type=\"weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11222500.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 638.65863037,  638.65863037], dtype=float32)"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#COVER\n",
    "p = u.predict_proba( X_train, ntree_limit=0)\n",
    "print(len(p)*sum(np.repeat(0.5,len(p))*(1-np.repeat(0.5,len(FF)))))  # sum of the hessians in that node,(root node has all data)\n",
    "FF = pd.DataFrame(X_train)\n",
    "FF['y'] =y_train\n",
    "FF['Prob0'] = pd.DataFrame(p)[0]\n",
    "FF['Prob1'] = pd.DataFrame(p)[1]\n",
    "#next node\n",
    "sum(np.array(FF[FF[1]<-0.225163][['Prob0','Prob1']])*(1-np.array(FF[FF[1]<-0.225163][['Prob0','Prob1']])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method XGBModel.get_params of XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
       "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=4,\n",
       "       min_child_weight=1, missing=None, n_estimators=5, nthread=-1,\n",
       "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=1)>"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = u.booster().get_dump( with_stats=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:[f1<-0.225163] yes=1,no=2,missing=1,gain=1853.31,cover=1675\n",
      "\t1:[f0<0.304833] yes=3,no=4,missing=3,gain=486.058,cover=694\n",
      "\t\t3:[f1<-0.645459] yes=7,no=8,missing=7,gain=57.4649,cover=442.25\n"
     ]
    }
   ],
   "source": [
    "#First Tree\n",
    "for tree in [results[0]]:\n",
    "    for line in tree.split('\\n'):\n",
    "        #if  'leaf=' in line:\n",
    "        print(line)\n",
    "        #print(line)\n",
    "        if '3:[' in line:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1853.31\n"
     ]
    }
   ],
   "source": [
    "##### Gain for first tree\n",
    "z = 0.5\n",
    "nn= -0.225163\n",
    "p = np.repeat(z,len(FF))\n",
    "#2783\n",
    "# L = which(X[,'odor=none']==0) \n",
    "# R = which(X[,'odor=none']==1)\n",
    "\n",
    "pL = np.repeat(z,len(FF[FF[1]<nn]['y'])) #p[FF[1]<-0.324049]\n",
    "pR = np.repeat(z,len(FF[FF[1]>=nn]['y'])) #p[FF[1]>= -0.324049]\n",
    "\n",
    "yL = FF[FF[1]<nn]['y']\n",
    "yR = FF[FF[1]>=nn]['y']\n",
    "\n",
    "GL = sum(pL-yL)\n",
    "GR = sum(pR-yR)\n",
    "G = sum(p-np.array(FF['y']))\n",
    "#G = sum(np.array(FF['Prob1'])-np.array(FF['y']))\n",
    "\n",
    "HL = sum(pL*(1-pL))\n",
    "HR = sum(pR*(1-pR))\n",
    "H = sum(p*(1-p))\n",
    "\n",
    "gain = 0.5*((GL**2/(HL+1))+(GR**2/(HR+1))-(G**2/(H+1)))\n",
    "print(round(gain*2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>Prob0</th>\n",
       "      <th>Prob1</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.545257</td>\n",
       "      <td>-0.653484</td>\n",
       "      <td>0.495375</td>\n",
       "      <td>0.504625</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.332258</td>\n",
       "      <td>1.866386</td>\n",
       "      <td>0.451967</td>\n",
       "      <td>0.548033</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.227723</td>\n",
       "      <td>-0.269642</td>\n",
       "      <td>0.466756</td>\n",
       "      <td>0.533244</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.006776</td>\n",
       "      <td>-0.993703</td>\n",
       "      <td>0.538894</td>\n",
       "      <td>0.461106</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.202238</td>\n",
       "      <td>0.914339</td>\n",
       "      <td>0.463489</td>\n",
       "      <td>0.536511</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1     Prob0     Prob1  y\n",
       "0  0.545257 -0.653484  0.495375  0.504625  0\n",
       "1 -0.332258  1.866386  0.451967  0.548033  1\n",
       "2  1.227723 -0.269642  0.466756  0.533244  1\n",
       "3 -0.006776 -0.993703  0.538894  0.461106  0\n",
       "4 -1.202238  0.914339  0.463489  0.536511  1"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Next tree\n",
    "tt = u.predict_proba(X_train,output_margin=False,ntree_limit=1)\n",
    "FF1 = pd.DataFrame(X_train)\n",
    "FF1['Prob0'] = pd.DataFrame(tt)[0]\n",
    "FF1['Prob1'] = pd.DataFrame(tt)[1]\n",
    "FF1['y'] =y_train\n",
    "FF1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:[f1<-0.225163] yes=1,no=2,missing=1,gain=1512.82,cover=1665.44\n",
      "\t1:[f0<0.304833] yes=3,no=4,missing=3,gain=397.685,cover=690.434\n",
      "\t\t3:[f1<-0.645459] yes=7,no=8,missing=7,gain=47.6899,cover=439.491\n"
     ]
    }
   ],
   "source": [
    "#Second Tree\n",
    "for tree in [results[1]]:\n",
    "    for line in tree.split('\\n'):\n",
    "        #if  'leaf=' in line:\n",
    "        print(line)\n",
    "        #print(line)\n",
    "        if '3:[' in line:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1512.82\n"
     ]
    }
   ],
   "source": [
    "########  GAIN of 2nd tree\n",
    "#Using prior trees probabilty Calculate the new G and H\n",
    "\n",
    "nn= -0.225163\n",
    "p = FF1['Prob1']\n",
    "\n",
    "pL = FF1[FF1[1]<nn]['Prob1']\n",
    "pR = FF1[FF1[1]>=nn]['Prob1']\n",
    "\n",
    "yL = FF1[FF1[1]<nn]['y']\n",
    "yR = FF1[FF1[1]>=nn]['y']\n",
    "\n",
    "GL = sum(pL-yL)\n",
    "GR = sum(pR-yR)\n",
    "G = sum(p-np.array(FF1['y']))\n",
    "\n",
    "HL = sum(pL*(1-pL))\n",
    "HR = sum(pR*(1-pR))\n",
    "H = sum(p*(1-p))\n",
    "\n",
    "gain = 0.5*((GL**2/(HL+1))+(GR**2/(HR+1))-(G**2/(H+1)))\n",
    "print(round(gain*2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###  XGBOOST RESOURCES\n",
    "\n",
    "#https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf\n",
    "#https://stackoverflow.com/questions/33520460/how-is-xgboost-cover-calculated/33614843\n",
    "# https://chaoticsenses.wordpress.com/2015/09/20/xgboost-a-macroscopic-anatomy/\n",
    "# http://www.grroups.com/blog/an-information-gain-based-feature-ranking-function-for-xgboost\n",
    "# https://machinelearningmastery.com/avoid-overfitting-by-early-stopping-with-xgboost-in-python/\n",
    "# https://cran.r-project.org/web/packages/xgboost/xgboost.pdf\n",
    "# https://machinelearningmastery.com/xgboost-python-mini-course/\n",
    "# https://machinelearningmastery.com/avoid-overfitting-by-early-stopping-with-xgboost-in-python/\n",
    "# https://stats.stackexchange.com/questions/231220/\n",
    "# how-to-compute-the-gradient-and-hessian-of-logarithmic-loss-question-is-based/231270\n",
    "# https://arxiv.org/pdf/1603.02754.pdf\n",
    "# https://arxiv.org/pdf/1606.05390.pdf\n",
    "# https://jessesw.com/XG-Boost/\n",
    "# https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "# https://datascience.stackexchange.com/questions/12318/how-do-i-interpret-the-output-of-xgboost-importance\n",
    "# https://stackoverflow.com/questions/33520460/how-is-xgboost-cover-calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 31, 'f0': 43} {'f1': 286.6727258064515, 'f0': 98.5243472883721} gain=\n"
     ]
    }
   ],
   "source": [
    "#### Calculate GAIN or COVER for the entire Model (ALL TREES)\n",
    "importance_type = 'gain' #'cover'\n",
    "importance_type += '='\n",
    "fmap = {}\n",
    "gmap = {}\n",
    "for tree in results:\n",
    "    for line in tree.split('\\n'):\n",
    "        # look for the opening square bracket\n",
    "        arr = line.split('[')\n",
    "        #print('arr: ',arr, len(arr))\n",
    "        # if no opening bracket (leaf node), ignore this line\n",
    "        if len(arr) == 1:\n",
    "            #print('arr: ',arr, len(arr))\n",
    "            continue\n",
    "\n",
    "        # look for the closing bracket, extract only info within that bracket\n",
    "        fid = arr[1].split(']')\n",
    "        #print('fid] : ',fid)\n",
    "        # extract gain or cover from string after closing bracket\n",
    "        g = float(fid[1].split(importance_type)[1].split(',')[0])\n",
    "        #print('g: ',g)\n",
    "        # extract feature name from string before closing bracket\n",
    "        fid = fid[0].split('<')[0]\n",
    "        #print('fid< : ',fid)\n",
    "\n",
    "        if fid not in fmap:\n",
    "            # if the feature hasn't been seen yet\n",
    "            fmap[fid] = 1\n",
    "            gmap[fid] = g\n",
    "        else:\n",
    "            fmap[fid] += 1\n",
    "            gmap[fid] += g\n",
    "\n",
    "# calculate average value (gain/cover) for each feature\n",
    "for fid in gmap:\n",
    "    gmap[fid] = gmap[fid] / fmap[fid]\n",
    "\n",
    "print(fmap,gmap,importance_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight {'f1': 6, 'f0': 9}\n"
     ]
    }
   ],
   "source": [
    "######  Calculate WEIGHT for the entire Model (ALL TREES)\n",
    "fmap={}\n",
    "for tree in [results[0]]:\n",
    "    for line in tree.split('\\n'):\n",
    "        # look for the opening square bracket\n",
    "        arr = line.split('[')\n",
    "        #print('arr :',arr,\"len(arr) == 1 : \",len(arr) == 1,len(arr))\n",
    "                    # if no opening bracket (leaf node), ignore this line\n",
    "        if len(arr) == 1:\n",
    "            #print(len(arr) == 1,'arr :',arr)\n",
    "            continue\n",
    "        # extract feature name from string between []\n",
    "        fid = arr[1].split(']')[0].split('<')[0]\n",
    "        #print('fid :',fid)\n",
    "        if fid not in fmap:\n",
    "        # if the feature hasn't been seen yet\n",
    "            fmap[fid] = 1\n",
    "        else:\n",
    "            fmap[fid] += 1\n",
    "print('Weight',fmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SplitValue</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.249088</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.141577</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.034065</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.926553</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.711530</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.604018</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.496506</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.388994</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.281483</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.173971</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.148564</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.471100</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.578611</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.116170</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SplitValue  Count\n",
       "0    -1.249088    2.0\n",
       "1    -1.141577    1.0\n",
       "2    -1.034065    1.0\n",
       "3    -0.926553    1.0\n",
       "4    -0.711530    4.0\n",
       "5    -0.604018    2.0\n",
       "6    -0.496506    1.0\n",
       "7    -0.388994    2.0\n",
       "8    -0.281483    1.0\n",
       "9    -0.173971    2.0\n",
       "10    0.148564    5.0\n",
       "11    0.471100    4.0\n",
       "12    0.578611    4.0\n",
       "13    1.116170    1.0"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u.booster().get_split_value_histogram('f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.581081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.418919</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    weights\n",
       "0  0.581081\n",
       "1  0.418919"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(u.feature_importances_, columns=['weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1cd00fdfb00>"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQwAAAD8CAYAAACCaZo+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cd7fa9b0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgb.plot_tree(u, num_trees=4, rankdir='LR') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def order_features_by_gains(bst, feature_map_file=None):\n",
    "    str_dump = bst.booster().get_dump(with_stats=True)\n",
    "    \n",
    "    tree_arr = []\n",
    "    for i_tree, tree in enumerate(str_dump):\n",
    "        arr_lvls=tree.split('\\n\\t')\n",
    "        a_tree = {}\n",
    "        for lvl in arr_lvls:\n",
    "            a_lvl ={}\n",
    "            dum1 = lvl.split(',')\n",
    "            if('leaf' in lvl):\n",
    "                dum1[0].replace('\\t','')\n",
    "                dum10 = dum1[0].split(':')\n",
    "                lvl_id = int(dum10[0])\n",
    "                dum11 = dum10[1].split('leaf=')\n",
    "                leaf = float(dum11[1])\n",
    "                \n",
    "                cover = float(dum1[1].replace('\\n','').split('cover=')[1])\n",
    "                a_lvl['lvl_id']=lvl_id\n",
    "                a_lvl['leaf']=leaf\n",
    "                a_lvl['cover']=cover\n",
    "            else:\n",
    "                dum10 = dum1[0].replace('\\t','').replace('\\n','')\n",
    "                dum11 = dum10.split(':')\n",
    "                lvl_id = int(dum11[0])\n",
    "                dum12 = dum11[1].split('yes=')\n",
    "                dum13 = dum12[0].replace('[','').replace(']','').split('<')\n",
    "                feat_name = dum13[0]\n",
    "                \n",
    "                yes_to = int(dum12[1])\n",
    "                no_to = int(dum1[1].split('no=')[1])\n",
    "                missing = int(dum1[2].split('missing=')[1])\n",
    "                gain = float(dum1[3].split('gain=')[1])\n",
    "                cover = float(dum1[4].split('cover=')[1])            \n",
    "                feat_thr = float(dum12[1])\n",
    "                \n",
    "                a_lvl['lvl_id']=lvl_id\n",
    "                a_lvl['feat_name']=feat_name\n",
    "                a_lvl['feat_thr'] = feat_thr\n",
    "                a_lvl['yes_to'] = yes_to\n",
    "                a_lvl['no_to']=no_to\n",
    "                a_lvl['missing'] = missing\n",
    "                a_lvl['gain']=gain\n",
    "                a_lvl['cover']=cover\n",
    "                \n",
    "            a_tree[str(lvl_id)] = a_lvl\n",
    "        tree_arr.append(a_tree)    \n",
    "    feat_vocabulary = {}\n",
    "    for tree in tree_arr:\n",
    "        for lvl in tree:\n",
    "            if('gain' in tree[lvl]):\n",
    "                feat_data = feat_vocabulary.setdefault(tree[lvl]['feat_name'],\\\n",
    "                                                       {'gain':tree[lvl]['gain'],'cover':tree[lvl]['cover']})\n",
    "                if(feat_data!={'gain':tree[lvl]['gain'],'cover':tree[lvl]['cover']}):\n",
    "                    try:\n",
    "                        feat_vocabulary[tree[lvl]['feat_name']]['gain'] += tree[lvl]['gain']                    \n",
    "                        feat_vocabulary[tree[lvl]['feat_name']]['cover'] += tree[lvl]['cover']\n",
    "                    except:\n",
    "                        feat_vocabulary[tree[lvl]['feat_name']]['gain'] = tree[lvl]['gain']                    \n",
    "                        feat_vocabulary[tree[lvl]['feat_name']]['cover'] = tree[lvl]['cover']          \n",
    "    \n",
    "    sorted_feats = sorted(feat_vocabulary.items(),key=lambda k:k[1]['gain'], reverse=True)\n",
    "    return sorted_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('f1', {'cover': 17844.449999999997, 'gain': 8886.8545}),\n",
       " ('f0', {'cover': 14460.2512, 'gain': 4236.546933399999})]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_features_by_gains(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate Data (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TOY DATA LOGISTIC Regression\n",
    "\n",
    "np.random.seed(seed=233423)\n",
    "x1 = st.norm.rvs(size=10000)           # some continuous variables \n",
    "x2 = st.norm.rvs(size=10000)  \n",
    "z = 1 + 2 *x1 +3 *x2        # linear combination with a bias\n",
    "pr = 1/(1 +np.exp(-z))         # pass through an inv-logit function\n",
    "y = st.binom.rvs(n=1,p=pr, size=10000) #rbinom(1000,1,pr) # bernoulli response variable\n",
    " \n",
    "X=np.column_stack([x1,x2])\n",
    "# standardize the features since regularization requires all features to be on same scale\n",
    "scaler = StandardScaler(copy=True)\n",
    "# we have created a standardization based on the training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaler.fit(X).transform(X), y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model=xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate=[0.0001,0.001,0.01,0.1,0.2,0.3]\n",
    "n_estimators=[50,100,150,200]\n",
    "max_depth=[2,4,6,8]\n",
    "param_grid=dict(learning_rate=learning_rate,max_depth=max_depth,n_estimators=n_estimators)\n",
    "kfold=StratifiedKFold(n_splits=10,shuffle=True,random_state=7)\n",
    "grid_search=GridSearchCV(model,param_grid,n_jobs=-1,cv=kfold)\n",
    "#grid_result=grid_search.fit(X,y)\n",
    "grid_result=grid_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.85452104,  0.85982983,  0.85490372])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Take forever\n",
    "cross_val_score(grid_result, X_train, y_train, cv=3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistic Regression Bayesian Inference approach :\n",
    "    - Pymc3 with Theano\n",
    "\n",
    "    Steps:\n",
    "        1. Prepare Data\n",
    "        2. Build Probabilistic Model\n",
    "        3. Condition Model on Data & Find the local maximum a posteriori point given a model (MAP)\n",
    "        4. Sample posterior distribution using MAP as starting points for Indepedent Variables(X's)\n",
    "        5. Generate posterior predictive samples from model given a Samples of posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1 Use theano shared variable so that we can make predictions for new values later\n",
    "log_dose_shared0 = shared(X_train[:, 0])\n",
    "log_dose_shared = shared(X_train[:, 1])\n",
    "\n",
    "# Sample size in each group. The sample size has to be a shared variable too\n",
    "# Each row/observation is a group so n = total in group. 1 if only one per group\n",
    "n_shared = shared(np.ones(len(X_train), dtype=int))\n",
    "\n",
    "# Outcomes/Target\n",
    "deaths = y_train\n",
    "\n",
    "\n",
    "# 2 Build Probabilistic Model\n",
    "with Model() as bioassay_model:\n",
    "\n",
    "    # Priors for unknown model parameters. e.g. Logit-linear model parameters\n",
    "    alpha = Normal('alpha', 0, sd=100)\n",
    "    beta = Normal('beta', 0, sd=100)\n",
    "    beta0 = Normal('beta0', 0, sd=100)\n",
    "\n",
    "    # Expected value of outcome. e.g. link function outcome. Calculate probabilities of Y/Target\n",
    "    theta = invlogit(alpha + beta * log_dose_shared + beta0 * log_dose_shared0 )\n",
    "\n",
    "    # Likelihood (sampling distribution) of observations Data likelihood YTarget\n",
    "    obs_deaths = Binomial('obs_deaths', n=n_shared, p=theta, observed=deaths)\n",
    "\n",
    "    \n",
    "# 3 Finds the local maximum a posteriori point given a model. uses BFGS.\n",
    "from pymc3 import find_MAP\n",
    "# Runs fit to data returns parameters/coefficients\n",
    "map_estimate = find_MAP(model=bioassay_model)\n",
    "print(map_estimate)\n",
    "\n",
    "\n",
    "# 4 Now draw samples from the posterior using the given step methods.\n",
    "with bioassay_model:\n",
    "    \n",
    "    # obtain starting values via MAP\n",
    "    start = find_MAP(model=bioassay_model)\n",
    "    \n",
    "    # instantiate sampler\n",
    "    step = pm.Metropolis()\n",
    "    \n",
    "    # posterior of X's\n",
    "    # draw 1,000 posterior samples of independent variables\n",
    "    bioassay_trace = sample(1000, step=step, start=start)\n",
    "\n",
    "\n",
    "# 5 Generate posterior predictive samples from a model given a trace.\n",
    "from pymc3 import sample_ppc\n",
    "\n",
    "with bioassay_model:\n",
    "    deaths_sim = sample_ppc(bioassay_trace, samples=1000)\n",
    "    \n",
    "# take only last half  of posterior distr. of X's. other half was burn in.\n",
    "tr1 = bioassay_trace[500:]\n",
    "    \n",
    "#PREDICT\n",
    "log_dose_to_predict0 = X_train[:1000,0] #np.random.uniform(-0.8,0.7,size=50)\n",
    "log_dose_to_predict = X_train[:1000,1] #np.random.uniform(-0.8,0.7,size=50)\n",
    "n_predict = n = np.ones(1000, dtype=int)\n",
    "\n",
    "# Changing values here will also change values in the model\n",
    "log_dose_shared0.set_value(log_dose_to_predict0)\n",
    "log_dose_shared.set_value(log_dose_to_predict)\n",
    "n_shared.set_value(n_predict)\n",
    "\n",
    "# Simply running PPC will use the updated values and do prediction\n",
    "ppc = pm.sample_ppc(tr1, model=bioassay_model, samples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print( 'Accuracy:',(ppc['obs_deaths']==y[:1000]).mean() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Changing values here will also change values in the model\n",
    "log_dose_shared0.set_value(np.array([ 0.93622719]))\n",
    "log_dose_shared.set_value(np.array([-1.2161206]))\n",
    "n_shared.set_value(np.array([1]))\n",
    "\n",
    "# Simply running PPC will use the updated values and do prediction\n",
    "ppc = pm.sample_ppc(tr1, model=bioassay_model, samples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logitInv= lambda x: np.exp(x)/(1.0+np.exp(x)) #sigmoid --> returns probability\n",
    "logitInv(0.9053831851570006 + 2.9168537609096776 * 0.08986007 + 1.9854704735996134 * 2.57440271)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(max_depth=100,max_features='sqrt', random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.feature_importances_)\n",
    "\n",
    "print( cross_val_score(clf, X_train, y_train, cv=10 ).mean() ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(clf.predict([[2.57440271,  0.08986007]]))\n",
    "clf.predict_proba([[2.57440271,  0.08986007]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression - Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#logistic Regression\n",
    "logit = LogisticRegression(fit_intercept=True)\n",
    "\n",
    "# Fit model. Let X_train = matrix of predictors, y_train = matrix of variable.\n",
    "# NOTE: Do not include a column for the intercept when fitting the model.\n",
    "resLogit = logit.fit(X_train, y_train)\n",
    "print(resLogit.coef_)\n",
    "print(cross_val_score(resLogit, X_train, y_train, cv=10 ).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print( cross_val_score(u, X_train, y_train, cv=10 ).mean() ,\n",
    "cross_val_score(clf, X_train, y_train, cv=10 ).mean(),\n",
    "cross_val_score(resLogit, X_train, y_train, cv=10 ).mean() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probs = u.predict_proba(X_train)\n",
    "preds = probs[:,1]\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_train, preds)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "pforest = clf.predict_proba(X_train)\n",
    "pforest1 = pforest[:,1]\n",
    "fpr1, tpr1, threshold1 = metrics.roc_curve(y_train, pforest1)\n",
    "roc_auc1 = metrics.auc(fpr1, tpr1)\n",
    "\n",
    "plog = logit.predict_proba(X_train)\n",
    "plog1 = plog[:,1]\n",
    "fpr2, tpr2, threshold2 = metrics.roc_curve(y_train, plog1)\n",
    "roc_auc2 = metrics.auc(fpr2, tpr2)\n",
    "\n",
    "# method I: plt\n",
    "#import matplotlib.pyplot as plt\n",
    "pyplot.title('Receiver Operating Characteristic')\n",
    "pyplot.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "pyplot.plot(fpr1, tpr1, 'g', label = 'AUC_f = %0.2f' % roc_auc1)\n",
    "pyplot.plot(fpr2, tpr2, 'y', label = 'AUC_L = %0.2f' % roc_auc2)\n",
    "pyplot.legend(loc = 'lower right')\n",
    "pyplot.plot([0, 1], [0, 1],'r--')\n",
    "pyplot.xlim([0, 1])\n",
    "pyplot.ylim([0, 1])\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate Data (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Toy Data Random Forest data\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=4,\n",
    "                            n_informative=2, n_redundant=0,\n",
    "                            random_state=0, shuffle=False)\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaler.fit(X).transform(X), y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(max_depth=100,max_features='sqrt', random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(clf.feature_importances_)\n",
    "\n",
    "print(cross_val_score(clf, X_train, y_train, cv=10 ),\n",
    "np.abs(cross_val_score(clf, X_train, y_train, cv=10 ).mean()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(clf.predict([[-0.57643759, -0.6723759 , -0.23639363,  0.54680607]]) , \n",
    "      clf.predict_proba([[-0.57643759, -0.6723759 , -0.23639363,  0.54680607]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression - Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeff:  [[ 1.96671609  2.9239234 ]]\n",
      "Acc:  [ 0.85245902  0.84925373  0.85820896  0.85522388  0.85373134  0.8641791\n",
      "  0.86268657  0.8641791   0.85671642  0.8490284 ] 0.856566652147\n"
     ]
    }
   ],
   "source": [
    "logit = LogisticRegression(fit_intercept=True)\n",
    "\n",
    "# Fit model. Let X_train = matrix of predictors, y_train = matrix of variable.\n",
    "# NOTE: Do not include a column for the intercept when fitting the model.\n",
    "resLogit = logit.fit(X_train, y_train)\n",
    "print('Coeff: ',resLogit.coef_)\n",
    "print('Acc: ',cross_val_score(resLogit, X_train, y_train, cv=10 )\n",
    "      , np.abs(cross_val_score(resLogit, X_train, y_train, cv=10 ).mean()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Sklearn vs Statsmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take aways:\n",
    "- Standard Error is the square root of the covariance matrix. Sklearn doesn't provide it.\n",
    "- Increase C in Sklearn to match Statsmodel output. Statsmodel isn't regularized and can't turn off Regularizer in Sklearn. It defaults to L2 and a high C negates regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard errors:  [ 0.04346896  0.05998815  0.07749697]\n",
      "Coefficients:     [ 0.93213188  1.98323743  2.94838434]\n"
     ]
    }
   ],
   "source": [
    "# Initiate logistic regression object\n",
    "logit = LogisticRegression(C=1e9,fit_intercept=True)\n",
    "\n",
    "# Fit model. Let X_train = matrix of predictors, y_train = matrix of variable.\n",
    "# NOTE: Do not include a column for the intercept when fitting the model.\n",
    "resLogit = logit.fit(X_train, y_train)\n",
    "#print(resLogit.intercept_,resLogit.coef_)\n",
    "\n",
    "# Calculate matrix of predicted class probabilities. \n",
    "# Check resLogit.classes_ to make sure that sklearn ordered your classes as expected\n",
    "predProbs = np.matrix(resLogit.predict_proba(X_train))\n",
    "\n",
    "# Design matrix -- add column of 1's at the beginning of your X_train matrix\n",
    "X_design = np.column_stack((np.ones(shape = X_train.shape[0]), X_train))\n",
    "#np.ones(shape = X_train.shape[0])\n",
    "#X_design =X_train\n",
    "\n",
    "# Initiate matrix of 0's, fill diagonal with each predicted observation's variance\n",
    "V = np.matrix(np.zeros(shape = (X_design.shape[0], X_design.shape[0])))\n",
    "np.fill_diagonal(V, np.multiply(predProbs[:,0], predProbs[:,1]).A1)\n",
    "\n",
    "# Covariance matrix\n",
    "covLogit = np.linalg.inv(X_design.T * V * X_design)\n",
    "#print(\"Covariance matrix: \", covLogit)\n",
    "\n",
    "# Standard errors\n",
    "print(\"Standard errors: \", np.sqrt(np.diag(covLogit)) )\n",
    "\n",
    "# Wald statistic (coefficient / s.e.) ^ 2\n",
    "logitParams = np.insert(resLogit.coef_, 0, resLogit.intercept_)\n",
    "print(\"Coefficients:    \",logitParams)\n",
    "#print( \"Wald statistics: \", (logitParams / np.sqrt(np.diag(covLogit))) ** 2)\n",
    "\n",
    "# Sklearn vs StatsModel Reference:\n",
    "# https://stackoverflow.com/questions/24924755/logit-estimator-in-statsmodels-and-sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.025</th>\n",
       "      <th>Coef.</th>\n",
       "      <th>0.975</th>\n",
       "      <th>St.Errors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Const.</th>\n",
       "      <td>0.847</td>\n",
       "      <td>0.932</td>\n",
       "      <td>1.017</td>\n",
       "      <td>0.043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x1</th>\n",
       "      <td>1.866</td>\n",
       "      <td>1.983</td>\n",
       "      <td>2.101</td>\n",
       "      <td>0.060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x2</th>\n",
       "      <td>2.796</td>\n",
       "      <td>2.948</td>\n",
       "      <td>3.100</td>\n",
       "      <td>0.077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0.025  Coef.  0.975  St.Errors\n",
       "Const.  0.847  0.932  1.017      0.043\n",
       "x1      1.866  1.983  2.101      0.060\n",
       "x2      2.796  2.948  3.100      0.077"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SKLEARN\n",
    "#print(\"Standard errors: \", np.sqrt(np.diag(covLogit)) )\n",
    "logitParams = np.insert(resLogit.coef_, 0, resLogit.intercept_)\n",
    "Intervals = pd.DataFrame({'0.975':[round(float(c+(1.96*v)),3) for c,v in zip(logitParams,np.sqrt(np.diag(covLogit)))],\n",
    "             'Coef.':[round(float(x),3) for x in logitParams],\n",
    "             '0.025':[round(float(c-(1.96*v)),3) for c,v in zip(logitParams,np.sqrt(np.diag(covLogit)))]}\n",
    "             ,index=['Const.','x1','x2'])\n",
    "Intervals['St.Errors']=[round(float(c),3) for c in np.sqrt(np.diag(covLogit))]\n",
    "Intervals[['0.025', 'Coef.', '0.975','St.Errors']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Logistic Regression - statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.310040\n",
      "         Iterations 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>y</td>        <th>  No. Observations:  </th>  <td>  6700</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>  <td>  6697</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>  <td>     2</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Tue, 14 Nov 2017</td> <th>  Pseudo R-squ.:     </th>  <td>0.5409</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>01:49:23</td>     <th>  Log-Likelihood:    </th> <td> -2077.3</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th> <td> -4524.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th>  <td> 0.000</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    0.9321</td> <td>    0.043</td> <td>   21.444</td> <td> 0.000</td> <td>    0.847</td> <td>    1.017</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    1.9832</td> <td>    0.060</td> <td>   33.060</td> <td> 0.000</td> <td>    1.866</td> <td>    2.101</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    2.9484</td> <td>    0.077</td> <td>   38.045</td> <td> 0.000</td> <td>    2.796</td> <td>    3.100</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   No. Observations:                 6700\n",
       "Model:                          Logit   Df Residuals:                     6697\n",
       "Method:                           MLE   Df Model:                            2\n",
       "Date:                Tue, 14 Nov 2017   Pseudo R-squ.:                  0.5409\n",
       "Time:                        01:49:23   Log-Likelihood:                -2077.3\n",
       "converged:                       True   LL-Null:                       -4524.5\n",
       "                                        LLR p-value:                     0.000\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          0.9321      0.043     21.444      0.000       0.847       1.017\n",
       "x1             1.9832      0.060     33.060      0.000       1.866       2.101\n",
       "x2             2.9484      0.077     38.045      0.000       2.796       3.100\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.formula.api as sm\n",
    "model = sm.Logit(y_train, X_design)\n",
    "result =model.fit() #model.fit(method='bfgs')\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistic Regression Bayesian Inference approach :\n",
    "    - Pymc3 with Theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1 Use theano shared variable so that we can make predictions for new values later\n",
    "log_dose_shared0 = shared(X_train[:, 0])\n",
    "log_dose_shared1 = shared(X_train[:, 1])\n",
    "log_dose_shared2 = shared(X_train[:, 2])\n",
    "log_dose_shared3 = shared(X_train[:, 3])\n",
    "\n",
    "# Sample size in each group. The sample size has to be a shared variable too\n",
    "# Each row/observation is a group so n = total in group. 1 if only one per group\n",
    "n_shared = shared(np.ones(len(X_train), dtype=int))\n",
    "\n",
    "# Outcomes/Target\n",
    "deaths = y_train\n",
    "\n",
    "\n",
    "# 2 Build Probabilistic Model\n",
    "with Model() as bioassay_model:\n",
    "\n",
    "    # Priors for unknown model parameters. e.g. Logit-linear model parameters\n",
    "    alpha = Normal('alpha', 0, sd=100)\n",
    "    beta0 = Normal('beta0', 0, sd=100)\n",
    "    beta1 = Normal('beta1', 0, sd=100)\n",
    "    beta2 = Normal('beta2', 0, sd=100)\n",
    "    beta3 = Normal('beta3', 0, sd=100)\n",
    "    \n",
    "    # Expected value of outcome. e.g. link function outcome. Calculate probabilities of Y/Target\n",
    "    theta = invlogit(alpha + beta0 * log_dose_shared0 + beta1 * log_dose_shared1\\\n",
    "                     + beta2 * log_dose_shared2 + beta3 * log_dose_shared3 )\n",
    "\n",
    "    # Likelihood (sampling distribution) of observations Data likelihood YTarget\n",
    "    obs_deaths = Binomial('obs_deaths', n=n_shared, p=theta, observed=deaths)\n",
    "\n",
    "    \n",
    "# 3 Finds the local maximum a posteriori point given a model. uses BFGS.\n",
    "from pymc3 import find_MAP\n",
    "# Runs fit to data returns parameters/coefficients\n",
    "map_estimate = find_MAP(model=bioassay_model)\n",
    "print(map_estimate)\n",
    "\n",
    "\n",
    "# 4 Now draw samples from the posterior using the given step methods.\n",
    "with bioassay_model:\n",
    "    \n",
    "    # obtain starting values via MAP\n",
    "    start = find_MAP(model=bioassay_model)\n",
    "    \n",
    "    # instantiate sampler\n",
    "    step = pm.Metropolis()\n",
    "    \n",
    "    # posterior of X's\n",
    "    # draw 1,000 posterior samples of independent variables\n",
    "    bioassay_trace = sample(1000, step=step, start=start)\n",
    "\n",
    "\n",
    "# 5 Generate posterior predictive samples from a model given a trace.\n",
    "from pymc3 import sample_ppc\n",
    "\n",
    "with bioassay_model:\n",
    "    deaths_sim = sample_ppc(bioassay_trace, samples=1000)\n",
    "    \n",
    "# take only last half  of posterior distr. of X's. other half was burn in.\n",
    "tr1 = bioassay_trace[500:]\n",
    "    \n",
    "#PREDICT\n",
    "log_dose_to_predict0 = X_train[:1000,0] #np.random.uniform(-0.8,0.7,size=50)\n",
    "log_dose_to_predict1 = X_train[:1000,1] #np.random.uniform(-0.8,0.7,size=50)\n",
    "log_dose_to_predict2 = X_train[:1000,2] #np.random.uniform(-0.8,0.7,size=50)\n",
    "log_dose_to_predict3 = X_train[:1000,3] #np.random.uniform(-0.8,0.7,size=50)\n",
    "n_predict = n = np.ones(1000, dtype=int)\n",
    "\n",
    "# Changing values here will also change values in the model\n",
    "log_dose_shared0.set_value(log_dose_to_predict0)\n",
    "log_dose_shared1.set_value(log_dose_to_predict1)\n",
    "log_dose_shared2.set_value(log_dose_to_predict2)\n",
    "log_dose_shared3.set_value(log_dose_to_predict3)\n",
    "n_shared.set_value(n_predict)\n",
    "\n",
    "# Simply running PPC will use the updated values and do prediction\n",
    "ppc = pm.sample_ppc(tr1, model=bioassay_model, samples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print( 'Accuracy:',(ppc['obs_deaths']==y[:1000]).mean() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_train[249:251],y_train[249:251]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Changing values here will also change values in the model\n",
    "log_dose_shared0.set_value(np.array([-0.57643759]))\n",
    "log_dose_shared1.set_value(np.array([-0.6723759]))\n",
    "log_dose_shared2.set_value(np.array([-0.23639363]))\n",
    "log_dose_shared3.set_value(np.array([0.54680607]))\n",
    "n_shared.set_value(np.array([1]))\n",
    "\n",
    "# Simply running PPC will use the updated values and do prediction\n",
    "ppc = pm.sample_ppc(tr1, model=bioassay_model, samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ppc['obs_deaths'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Changing values here will also change values in the model\n",
    "log_dose_shared0.set_value(np.array([-0.98566996]))\n",
    "log_dose_shared1.set_value(np.array([1.12344181]))\n",
    "log_dose_shared2.set_value(np.array([-0.35003196]))\n",
    "log_dose_shared3.set_value(np.array([-1.1158904]))\n",
    "n_shared.set_value(np.array([1]))\n",
    "\n",
    "# Simply running PPC will use the updated values and do prediction\n",
    "ppc = pm.sample_ppc(tr1, model=bioassay_model, samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ppc['obs_deaths'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "map_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logitInv= lambda x: np.exp(x)/(1.0+np.exp(x)) #sigmoid --> returns probability\n",
    "logitInv(map_estimate['alpha']+map_estimate['beta0']*X_train[249:251][1][0]+\\\n",
    "         map_estimate['beta1']*X_train[249:251][1][1]\\\n",
    "+map_estimate['beta2']*X_train[249:251][1][2]+map_estimate['beta3']*X_train[249:251][1][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import random, math\n",
    "\n",
    "# def k_fold(data, myseed=11109, k=10):\n",
    "#     # Load data\n",
    "#     #data = open(myfile).readlines()\n",
    "\n",
    "#     # Shuffle input\n",
    "#     random.seed=myseed\n",
    "#     random.shuffle(data)\n",
    "\n",
    "#     # Compute partition size given input k\n",
    "#     len_part=int(math.ceil(len(data)/float(k)))\n",
    "\n",
    "#     # Create one partition per fold\n",
    "#     train={}\n",
    "#     test={}\n",
    "#     for ii in range(k):\n",
    "#         test[ii]  = data[ii*len_part:ii*len_part+len_part]\n",
    "#         train[ii] = [jj for jj in data if jj not in test[ii]]\n",
    "\n",
    "#     return train, test \n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# plt.errorbar(x=log_dose_to_predict0[:50], \\\n",
    "# y=np.asarray(ppc['obs_deaths']).mean(axis=0)[:50],\\\n",
    "# yerr=np.asarray(ppc['obs_deaths']).std(axis=0)[:50], linestyle='', marker='o')\n",
    "# plt.plot(X_train[:50, 1], deaths[:50], 'o')\n",
    "# plt.xlabel('log_dose',size=15)s\n",
    "# plt.ylabel('number of rats with tumors',size=15)\n",
    "\n",
    "#now feed it to glm:\n",
    "#df = data.frame(y=y,x1=x1,x2=x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rf = RandomForestRegressor(n_estimators = 100, max_features='sqrt')\n",
    "# rf.fit(X, y)\n",
    "# # feature importances\n",
    "# # the higher, the more important the feature\n",
    "# d = {'importance': rf.feature_importances_}\n",
    "# pd.DataFrame(d, index=X.columns).sort('importance')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
