{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.stats as st \n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from patsy import dmatrices\n",
    "from sklearn.ensemble.forest import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "from pymc3 import Normal, Binomial, sample, Model # Import relevant distributions\n",
    "from pymc3.math import invlogit\n",
    "# Use a theano shared variable to be able to exchange the data the model runs on\n",
    "from theano import shared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hvill\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib import pyplot\n",
    "import xgboost as xgb\n",
    "# from xgboost import XGBClassifier\n",
    "# from xgboost import plot_importance\n",
    "# from xgboost import plot_tree\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TOY DATA LOGISTIC Regression\n",
    "#> set.seed(666)\n",
    "np.random.seed(seed=233423)\n",
    "x1 = st.norm.rvs(size=10000)           # some continuous variables \n",
    "x2 = st.norm.rvs(size=10000)  \n",
    "z = 1 + 2 *x1 +3 *x2        # linear combination with a bias\n",
    "pr = 1/(1 +np.exp(-z))         # pass through an inv-logit function\n",
    "y = st.binom.rvs(n=1,p=pr, size=10000) #rbinom(1000,1,pr) # bernoulli response variable\n",
    " \n",
    "X=np.column_stack([x1,x2])\n",
    "# standardize the features since regularization requires all features to be on same scale\n",
    "scaler = StandardScaler(copy=True)\n",
    "# we have created a standardization based on the training data\n",
    "X_train = scaler.fit(X).transform(X)\n",
    "y_train = y\n",
    "\n",
    "#now feed it to glm:\n",
    "#df = data.frame(y=y,x1=x1,x2=x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u.feature_importances_ [ 0.60000002  0.40000001]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.84740233660233655"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u=xgb.XGBClassifier()\n",
    "\n",
    "u.set_params( learning_rate= 0.1, max_depth= 4, n_estimators= 5 )\n",
    "u.fit(X_train, y_train)\n",
    "\n",
    "print('u.feature_importances_',u.feature_importances_)\n",
    "cross_val_score(u, X_train, y_train, cv=10 ).mean()\n",
    "\n",
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1cd7f556c18>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEWCAYAAACOv5f1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cd7f4a3c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgb.plot_importance(u,importance_type=\"weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f0': 513.51904, 'f1': 852.4843666666667}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u.booster().get_score(importance_type='cover')\n",
    "# self, fmap='', importance_type='weight'):\n",
    "# \"\"\"Get feature importance of each feature.\n",
    "# Importance type can be defined as:\n",
    "# 'weight' - the number of times a feature is used to split the data across all trees.\n",
    "# 'gain' - the average gain of the feature when it is used in trees\n",
    "# 'cover' - the average coverage of the feature when it is used in trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = u.booster().get_dump( with_stats=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.714387</td>\n",
       "      <td>1.043728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.443381</td>\n",
       "      <td>2.914634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.209301</td>\n",
       "      <td>1.064722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.574403</td>\n",
       "      <td>0.089860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.936227</td>\n",
       "      <td>-1.216121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1\n",
       "0 -0.714387  1.043728\n",
       "1  0.443381  2.914634\n",
       "2  0.209301  1.064722\n",
       "3  2.574403  0.089860\n",
       "4  0.936227 -1.216121"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FF = pd.DataFrame(X_train)\n",
    "FF['y'] =y_train\n",
    "len(FF['y'])*(.5)*(1 - .5)# = 1628.25\n",
    "len(FF[(FF[0]<0.304833) & (FF[1]<-0.225163)])\n",
    "FF[[0,1]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method XGBModel.get_params of XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
       "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=4,\n",
       "       min_child_weight=1, missing=None, n_estimators=5, nthread=-1,\n",
       "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=1)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = u.predict_proba( X_train, ntree_limit=3)\n",
    "#pd.DataFrame(t)[1].value_counts()\n",
    "tt = u.predict_proba(X_train,output_margin=False,ntree_limit=1)\n",
    "\n",
    "#head(p)\n",
    "#[1] 0.8471184 0.1544077 0.1544077 0.8471184 0.1255700 0.1544077\n",
    "#FF1 = pd.DataFrame(tt)#[1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.4669106 ,  0.5330894 ],\n",
       "       [ 0.45284617,  0.54715383],\n",
       "       [ 0.45284617,  0.54715383],\n",
       "       ..., \n",
       "       [ 0.45284617,  0.54715383],\n",
       "       [ 0.45284617,  0.54715383],\n",
       "       [ 0.45284617,  0.54715383]], dtype=float32)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u.predict_proba(X_train,output_margin=False, ntree_limit=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 879.11950684,  879.11950684], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#COVER\n",
    "p = u.predict_proba( X_train, ntree_limit=4)\n",
    "sum(p*(1-p))  # sum of the hessians in that node,(root node has all data)\n",
    "FF['Prob0'] = pd.DataFrame(p)[0]\n",
    "FF['Prob1'] = pd.DataFrame(p)[1]\n",
    "#next node\n",
    "sum(np.array(FF[FF[1]<-0.324049][['Prob0','Prob1']])*(1-np.array(FF[FF[1]<-0.324049][['Prob0','Prob1']])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>y</th>\n",
       "      <th>Prob0</th>\n",
       "      <th>Prob1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.714387</td>\n",
       "      <td>1.043728</td>\n",
       "      <td>1</td>\n",
       "      <td>0.381357</td>\n",
       "      <td>0.618643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.443381</td>\n",
       "      <td>2.914634</td>\n",
       "      <td>1</td>\n",
       "      <td>0.336233</td>\n",
       "      <td>0.663767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.209301</td>\n",
       "      <td>1.064722</td>\n",
       "      <td>1</td>\n",
       "      <td>0.336233</td>\n",
       "      <td>0.663767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.574403</td>\n",
       "      <td>0.089860</td>\n",
       "      <td>1</td>\n",
       "      <td>0.359606</td>\n",
       "      <td>0.640394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.936227</td>\n",
       "      <td>-1.216121</td>\n",
       "      <td>0</td>\n",
       "      <td>0.537848</td>\n",
       "      <td>0.462151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1  y     Prob0     Prob1\n",
       "0 -0.714387  1.043728  1  0.381357  0.618643\n",
       "1  0.443381  2.914634  1  0.336233  0.663767\n",
       "2  0.209301  1.064722  1  0.336233  0.663767\n",
       "3  2.574403  0.089860  1  0.359606  0.640394\n",
       "4  0.936227 -1.216121  0  0.537848  0.462151"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
    "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=4,\n",
    "       min_child_weight=1, missing=None, n_estimators=5, nthread=-1,\n",
    "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
    "       scale_pos_weight=1, seed=0, silent=True, subsample=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "31*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512.86044257"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2782.81044257-2269.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:[f1<-0.216348] yes=1,no=2,missing=1,gain=2269.95,cover=2485.74\n",
      "\t1:[f0<0.589985] yes=3,no=4,missing=3,gain=587.622,cover=1032.7\n",
      "\t\t3:[f1<-0.671597] yes=7,no=8,missing=7,gain=109.089,cover=751.703\n",
      "\t\t\t7:[f0<0.188756] yes=15,no=16,missing=15,gain=18.5527,cover=452.69\n",
      "\t\t\t\t15:leaf=-0.1675,cover=363.314\n",
      "\t\t\t\t16:leaf=-0.113627,cover=89.376\n",
      "\t\t\t8:[f0<-0.210408] yes=17,no=18,missing=17,gain=98.56,cover=299.013\n",
      "\t\t\t\t17:leaf=-0.126543,cover=176.285\n",
      "\t\t\t\t18:leaf=-0.00987995,cover=122.728\n",
      "\t\t4:[f1<-1.32155] yes=9,no=10,missing=9,gain=210.328,cover=281\n",
      "\t\t\t9:[f0<1.61126] yes=19,no=20,missing=19,gain=26.8713,cover=61.0972\n",
      "\t\t\t\t19:leaf=-0.153455,cover=48.1134\n",
      "\t\t\t\t20:leaf=0.00773177,cover=12.9838\n",
      "\t\t\t10:[f1<-0.743804] yes=21,no=22,missing=21,gain=54.9026,cover=219.903\n",
      "\t\t\t\t21:leaf=0.0285418,cover=90.4539\n",
      "\t\t\t\t22:leaf=0.130316,cover=129.449\n",
      "\t2:[f0<-0.621509] yes=5,no=6,missing=5,gain=487.727,cover=1453.04\n",
      "\t\t5:[f1<0.546479] yes=11,no=12,missing=11,gain=272.5,cover=379.128\n",
      "\t\t\t11:[f1<0.122672] yes=23,no=24,missing=23,gain=32.9827,cover=190.39\n",
      "\t\t\t\t23:leaf=-0.115626,cover=81.8426\n",
      "\t\t\t\t24:leaf=-0.0314246,cover=108.547\n",
      "\t\t\t12:[f0<-1.8802] yes=25,no=26,missing=25,gain=56.8101,cover=188.739\n",
      "\t\t\t\t25:leaf=-0.0560621,cover=19.9809\n",
      "\t\t\t\t26:leaf=0.119915,cover=168.758\n",
      "\t\t6:[f1<0.118593] yes=13,no=14,missing=13,gain=103.077,cover=1073.91\n",
      "\t\t\t13:[f0<0.152216] yes=27,no=28,missing=27,gain=105.046,cover=239.33\n",
      "\t\t\t\t27:leaf=0.010914,cover=99.4711\n",
      "\t\t\t\t28:leaf=0.145263,cover=139.859\n",
      "\t\t\t14:[f0<-0.123068] yes=29,no=30,missing=29,gain=18.7487,cover=834.58\n",
      "\t\t\t\t29:leaf=0.136891,cover=208.449\n",
      "\t\t\t\t30:leaf=0.173862,cover=626.131\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for tree in [results[1]]:\n",
    "    for line in tree.split('\\n'):\n",
    "        #if  'leaf=' in line:\n",
    "            print(line)\n",
    "        #print(line)\n",
    "#         if '7:[' in line:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2782.51368475 2783.62669857 336.954818073\n"
     ]
    }
   ],
   "source": [
    "#####   Gain for first tree\n",
    "z = 0.5 #0.4948294268243327\n",
    "nn= -0.216348\n",
    "p = np.repeat(z,len(FF))\n",
    "#2783\n",
    "# L = which(X[,'odor=none']==0) \n",
    "# R = which(X[,'odor=none']==1)\n",
    "\n",
    "pL = np.repeat(z,len(FF[FF[1]<nn]['y'])) #p[FF[1]<-0.324049]\n",
    "pR = np.repeat(z,len(FF[FF[1]>=nn]['y'])) #p[FF[1]>= -0.324049]\n",
    "\n",
    "yL = FF[FF[1]<nn]['y']\n",
    "yR = FF[FF[1]>=nn]['y']\n",
    "\n",
    "GL = sum(pL-yL)\n",
    "GR = sum(pR-yR)\n",
    "G = sum(p-np.array(FF['y']))\n",
    "#G = sum(np.array(FF['Prob1'])-np.array(FF['y']))\n",
    "\n",
    "HL = sum(pL*(1-pL))\n",
    "HR = sum(pR*(1-pR))\n",
    "H = sum(p*(1-p))\n",
    "\n",
    "gain = 0.5*((GL**2/(HL+1))+(GR**2/(HR+1))-(G**2/(H+1)))\n",
    "print(gain*2,gain+0.5*((GL**2/(HL))+(GR**2/(HR))-(G**2/(H))),(G**2/(H+1)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>Prob0</th>\n",
       "      <th>Prob1</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.714387</td>\n",
       "      <td>1.043728</td>\n",
       "      <td>0.466911</td>\n",
       "      <td>0.533089</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.443381</td>\n",
       "      <td>2.914634</td>\n",
       "      <td>0.452846</td>\n",
       "      <td>0.547154</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.209301</td>\n",
       "      <td>1.064722</td>\n",
       "      <td>0.452846</td>\n",
       "      <td>0.547154</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.574403</td>\n",
       "      <td>0.089860</td>\n",
       "      <td>0.459199</td>\n",
       "      <td>0.540801</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.936227</td>\n",
       "      <td>-1.216121</td>\n",
       "      <td>0.526546</td>\n",
       "      <td>0.473454</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1     Prob0     Prob1  y\n",
       "0 -0.714387  1.043728  0.466911  0.533089  1\n",
       "1  0.443381  2.914634  0.452846  0.547154  1\n",
       "2  0.209301  1.064722  0.452846  0.547154  1\n",
       "3  2.574403  0.089860  0.459199  0.540801  1\n",
       "4  0.936227 -1.216121  0.526546  0.473454  0"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Next tree\n",
    "tt = u.predict_proba(X_train,output_margin=False,ntree_limit=1)\n",
    "FF1 = pd.DataFrame(X_train)\n",
    "FF1['Prob0'] = pd.DataFrame(tt)[0]\n",
    "FF1['Prob1'] = pd.DataFrame(tt)[1]\n",
    "FF1['y'] =y_train\n",
    "FF1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2269.9502252 2270.86342417 274.73709074\n"
     ]
    }
   ],
   "source": [
    "###########GAIN of 2nd tree\n",
    "#Using prior trees probabilty the is new G and H\n",
    "#getting gain to work\n",
    "# z = 0.4948294268243327\n",
    "nn= -0.216348\n",
    "p = FF1['Prob1']\n",
    "#2783\n",
    "# L = which(X[,'odor=none']==0) \n",
    "# R = which(X[,'odor=none']==1)\n",
    "#FF1[FF1[1]<nn]['Prob1']\n",
    "\n",
    "pL = FF1[FF1[1]<nn]['Prob1']\n",
    "pR = FF1[FF1[1]>=nn]['Prob1']\n",
    "\n",
    "yL = FF1[FF1[1]<nn]['y']\n",
    "yR = FF1[FF1[1]>=nn]['y']\n",
    "\n",
    "GL = sum(pL-yL)\n",
    "GR = sum(pR-yR)\n",
    "G = sum(p-np.array(FF1['y']))\n",
    "#G = sum(np.array(FF['Prob1'])-np.array(FF['y']))\n",
    "\n",
    "HL = sum(pL*(1-pL))\n",
    "HR = sum(pR*(1-pR))\n",
    "H = sum(p*(1-p))\n",
    "\n",
    "gain = 0.5*((GL**2/(HL+1))+(GR**2/(HR+1))-(G**2/(H+1)))\n",
    "print(gain*2,gain+0.5*((GL**2/(HL))+(GR**2/(HR))-(G**2/(H))),(G**2/(H+1)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-826.559224784 2485.74159807\n"
     ]
    }
   ],
   "source": [
    "print(G,H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 30, 'f0': 45} {'f1': 436.32703499999997, 'f0': 141.75926555555554} gain=\n"
     ]
    }
   ],
   "source": [
    "importance_type = 'gain' #'cover'\n",
    "importance_type += '='\n",
    "fmap = {}\n",
    "gmap = {}\n",
    "for tree in results:\n",
    "    for line in tree.split('\\n'):\n",
    "        # look for the opening square bracket\n",
    "        arr = line.split('[')\n",
    "        #print('arr: ',arr, len(arr))\n",
    "        # if no opening bracket (leaf node), ignore this line\n",
    "        if len(arr) == 1:\n",
    "            #print('arr: ',arr, len(arr))\n",
    "            continue\n",
    "\n",
    "        # look for the closing bracket, extract only info within that bracket\n",
    "        fid = arr[1].split(']')\n",
    "        #print('fid] : ',fid)\n",
    "        # extract gain or cover from string after closing bracket\n",
    "        g = float(fid[1].split(importance_type)[1].split(',')[0])\n",
    "        #print('g: ',g)\n",
    "        # extract feature name from string before closing bracket\n",
    "        fid = fid[0].split('<')[0]\n",
    "        #print('fid< : ',fid)\n",
    "\n",
    "        if fid not in fmap:\n",
    "            # if the feature hasn't been seen yet\n",
    "            fmap[fid] = 1\n",
    "            gmap[fid] = g\n",
    "        else:\n",
    "            fmap[fid] += 1\n",
    "            gmap[fid] += g\n",
    "\n",
    "# calculate average value (gain/cover) for each feature\n",
    "for fid in gmap:\n",
    "    gmap[fid] = gmap[fid] / fmap[fid]\n",
    "\n",
    "print(fmap,gmap,importance_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight {'f1': 5, 'f0': 10}\n"
     ]
    }
   ],
   "source": [
    "################ WEIGHT\n",
    "#results\n",
    "fmap={}\n",
    "for tree in [results[0]]:\n",
    "    for line in tree.split('\\n'):\n",
    "        # look for the opening square bracket\n",
    "        arr = line.split('[')\n",
    "        #print('arr :',arr,\"len(arr) == 1 : \",len(arr) == 1,len(arr))\n",
    "                    # if no opening bracket (leaf node), ignore this line\n",
    "        if len(arr) == 1:\n",
    "            #print(len(arr) == 1,'arr :',arr)\n",
    "            continue\n",
    "        # extract feature name from string between []\n",
    "        fid = arr[1].split(']')[0].split('<')[0]\n",
    "        #print('fid :',fid)\n",
    "        if fid not in fmap:\n",
    "        # if the feature hasn't been seen yet\n",
    "            fmap[fid] = 1\n",
    "        else:\n",
    "            fmap[fid] += 1\n",
    "print('Weight',fmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SplitValue</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.222429</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.123308</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.925066</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.726824</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.627703</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.528582</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.231219</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.132098</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.165265</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.462628</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.561749</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SplitValue  Count\n",
       "0    -1.222429    3.0\n",
       "1    -1.123308    1.0\n",
       "2    -0.925066    2.0\n",
       "3    -0.726824    3.0\n",
       "4    -0.627703    2.0\n",
       "5    -0.528582    1.0\n",
       "6    -0.231219    2.0\n",
       "7    -0.132098    3.0\n",
       "8     0.165265    7.0\n",
       "9     0.462628    1.0\n",
       "10    0.561749    5.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u.booster().get_split_value_histogram('f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   weights\n",
       "0      0.6\n",
       "1      0.4"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pd.DataFrame(, columns=['feature','importance'])#.sort_values('importance', ascending=False)\n",
    "#u.booster().get_fscore().items()\n",
    "pd.DataFrame(u.feature_importances_, columns=['weights'])#, index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1cd7fa0d7f0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQgAAAD8CAYAAACLgjpEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkfX/WCvV9WeftO4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cd7fa009b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgb.plot_tree(u, num_trees=4, rankdir='LR') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('f1', {'cover': 25574.530999999995, 'gain': 13089.811049999998}),\n",
       " ('f0', {'cover': 23108.356800000005, 'gain': 6379.166949999999})]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_features_by_gains(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_features_by_gains(bst, feature_map_file=None):\n",
    "    str_dump = bst.booster().get_dump(with_stats=True)\n",
    "    \n",
    "    tree_arr = []\n",
    "    for i_tree, tree in enumerate(str_dump):\n",
    "        arr_lvls=tree.split('\\n\\t')\n",
    "        a_tree = {}\n",
    "        for lvl in arr_lvls:\n",
    "            a_lvl ={}\n",
    "            dum1 = lvl.split(',')\n",
    "            if('leaf' in lvl):\n",
    "                dum1[0].replace('\\t','')\n",
    "                dum10 = dum1[0].split(':')\n",
    "                lvl_id = int(dum10[0])\n",
    "                dum11 = dum10[1].split('leaf=')\n",
    "                leaf = float(dum11[1])\n",
    "                \n",
    "                cover = float(dum1[1].replace('\\n','').split('cover=')[1])\n",
    "                a_lvl['lvl_id']=lvl_id\n",
    "                a_lvl['leaf']=leaf\n",
    "                a_lvl['cover']=cover\n",
    "            else:\n",
    "                dum10 = dum1[0].replace('\\t','').replace('\\n','')\n",
    "                dum11 = dum10.split(':')\n",
    "                lvl_id = int(dum11[0])\n",
    "                dum12 = dum11[1].split('yes=')\n",
    "                dum13 = dum12[0].replace('[','').replace(']','').split('<')\n",
    "                feat_name = dum13[0]\n",
    "                \n",
    "                yes_to = int(dum12[1])\n",
    "                no_to = int(dum1[1].split('no=')[1])\n",
    "                missing = int(dum1[2].split('missing=')[1])\n",
    "                gain = float(dum1[3].split('gain=')[1])\n",
    "                cover = float(dum1[4].split('cover=')[1])            \n",
    "                feat_thr = float(dum12[1])\n",
    "                \n",
    "                a_lvl['lvl_id']=lvl_id\n",
    "                a_lvl['feat_name']=feat_name\n",
    "                a_lvl['feat_thr'] = feat_thr\n",
    "                a_lvl['yes_to'] = yes_to\n",
    "                a_lvl['no_to']=no_to\n",
    "                a_lvl['missing'] = missing\n",
    "                a_lvl['gain']=gain\n",
    "                a_lvl['cover']=cover\n",
    "                \n",
    "            a_tree[str(lvl_id)] = a_lvl\n",
    "        tree_arr.append(a_tree)    \n",
    "    feat_vocabulary = {}\n",
    "    for tree in tree_arr:\n",
    "        for lvl in tree:\n",
    "            if('gain' in tree[lvl]):\n",
    "                feat_data = feat_vocabulary.setdefault(tree[lvl]['feat_name'],{'gain':tree[lvl]['gain'],'cover':tree[lvl]['cover']})\n",
    "                if(feat_data!={'gain':tree[lvl]['gain'],'cover':tree[lvl]['cover']}):\n",
    "                    try:\n",
    "                        feat_vocabulary[tree[lvl]['feat_name']]['gain'] += tree[lvl]['gain']                    \n",
    "                        feat_vocabulary[tree[lvl]['feat_name']]['cover'] += tree[lvl]['cover']\n",
    "                    except:\n",
    "                        feat_vocabulary[tree[lvl]['feat_name']]['gain'] = tree[lvl]['gain']                    \n",
    "                        feat_vocabulary[tree[lvl]['feat_name']]['cover'] = tree[lvl]['cover']          \n",
    "    \n",
    "    sorted_feats = sorted(feat_vocabulary.items(),key=lambda k:k[1]['gain'], reverse=True)\n",
    "    return sorted_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate Data (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TOY DATA LOGISTIC Regression\n",
    "#> set.seed(666)\n",
    "np.random.seed(seed=233423)\n",
    "x1 = st.norm.rvs(size=10000)           # some continuous variables \n",
    "x2 = st.norm.rvs(size=10000)  \n",
    "z = 1 + 2 *x1 +3 *x2        # linear combination with a bias\n",
    "pr = 1/(1 +np.exp(-z))         # pass through an inv-logit function\n",
    "y = st.binom.rvs(n=1,p=pr, size=10000) #rbinom(1000,1,pr) # bernoulli response variable\n",
    " \n",
    "X=np.column_stack([x1,x2])\n",
    "# standardize the features since regularization requires all features to be on same scale\n",
    "scaler = StandardScaler(copy=True)\n",
    "# we have created a standardization based on the training data\n",
    "X_train = scaler.fit(X).transform(X)\n",
    "y_train = y\n",
    "\n",
    "#now feed it to glm:\n",
    "#df = data.frame(y=y,x1=x1,x2=x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaler.fit(X).transform(X), y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model=XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate=[0.0001,0.001,0.01,0.1,0.2,0.3]\n",
    "n_estimators=[50,100,150,200]\n",
    "max_depth=[2,4,6,8]\n",
    "param_grid=dict(learning_rate=learning_rate,max_depth=max_depth,n_estimators=n_estimators)\n",
    "kfold=StratifiedKFold(n_splits=10,shuffle=True,random_state=7)\n",
    "grid_search=GridSearchCV(model,param_grid,scoring='neg_log_loss',n_jobs=-1,cv=kfold)\n",
    "#grid_result=grid_search.fit(X,y)\n",
    "grid_result=grid_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Take forever\n",
    "#cross_val_score(grid_result, X_train, y_train, cv=10 ).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistic Regression Bayesian Inference approach :\n",
    "    - Pymc3 with Theano\n",
    "\n",
    "    Steps:\n",
    "        1. Prepare Data\n",
    "        2. Build Probabilistic Model\n",
    "        3. Condition Model on Data & Find the local maximum a posteriori point given a model (MAP)\n",
    "        4. Sample posterior distribution using MAP as starting points for Indepedent Variables(X's)\n",
    "        5. Generate posterior predictive samples from model given a Samples of posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Use theano shared variable so that we can make predictions for new values later\n",
    "log_dose_shared0 = shared(X_train[:, 0])\n",
    "log_dose_shared = shared(X_train[:, 1])\n",
    "\n",
    "# Sample size in each group. The sample size has to be a shared variable too\n",
    "# Each row/observation is a group so n = total in group. 1 if only one per group\n",
    "n_shared = shared(np.ones(len(X_train), dtype=int))\n",
    "\n",
    "# Outcomes/Target\n",
    "deaths = y_train\n",
    "\n",
    "\n",
    "# 2 Build Probabilistic Model\n",
    "with Model() as bioassay_model:\n",
    "\n",
    "    # Priors for unknown model parameters. e.g. Logit-linear model parameters\n",
    "    alpha = Normal('alpha', 0, sd=100)\n",
    "    beta = Normal('beta', 0, sd=100)\n",
    "    beta0 = Normal('beta0', 0, sd=100)\n",
    "\n",
    "    # Expected value of outcome. e.g. link function outcome. Calculate probabilities of Y/Target\n",
    "    theta = invlogit(alpha + beta * log_dose_shared + beta0 * log_dose_shared0 )\n",
    "\n",
    "    # Likelihood (sampling distribution) of observations Data likelihood YTarget\n",
    "    obs_deaths = Binomial('obs_deaths', n=n_shared, p=theta, observed=deaths)\n",
    "\n",
    "    \n",
    "# 3 Finds the local maximum a posteriori point given a model. uses BFGS.\n",
    "from pymc3 import find_MAP\n",
    "# Runs fit to data returns parameters/coefficients\n",
    "map_estimate = find_MAP(model=bioassay_model)\n",
    "print(map_estimate)\n",
    "\n",
    "\n",
    "# 4 Now draw samples from the posterior using the given step methods.\n",
    "with bioassay_model:\n",
    "    \n",
    "    # obtain starting values via MAP\n",
    "    start = find_MAP(model=bioassay_model)\n",
    "    \n",
    "    # instantiate sampler\n",
    "    step = pm.Metropolis()\n",
    "    \n",
    "    # posterior of X's\n",
    "    # draw 1,000 posterior samples of independent variables\n",
    "    bioassay_trace = sample(1000, step=step, start=start)\n",
    "\n",
    "\n",
    "# 5 Generate posterior predictive samples from a model given a trace.\n",
    "from pymc3 import sample_ppc\n",
    "\n",
    "with bioassay_model:\n",
    "    deaths_sim = sample_ppc(bioassay_trace, samples=1000)\n",
    "    \n",
    "# take only last half  of posterior distr. of X's. other half was burn in.\n",
    "tr1 = bioassay_trace[500:]\n",
    "    \n",
    "#PREDICT\n",
    "log_dose_to_predict0 = X_train[:1000,0] #np.random.uniform(-0.8,0.7,size=50)\n",
    "log_dose_to_predict = X_train[:1000,1] #np.random.uniform(-0.8,0.7,size=50)\n",
    "n_predict = n = np.ones(1000, dtype=int)\n",
    "\n",
    "# Changing values here will also change values in the model\n",
    "log_dose_shared0.set_value(log_dose_to_predict0)\n",
    "log_dose_shared.set_value(log_dose_to_predict)\n",
    "n_shared.set_value(n_predict)\n",
    "\n",
    "# Simply running PPC will use the updated values and do prediction\n",
    "ppc = pm.sample_ppc(tr1, model=bioassay_model, samples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( 'Accuracy:',(ppc['obs_deaths']==y[:1000]).mean() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing values here will also change values in the model\n",
    "log_dose_shared0.set_value(np.array([ 0.93622719]))\n",
    "log_dose_shared.set_value(np.array([-1.2161206]))\n",
    "n_shared.set_value(np.array([1]))\n",
    "\n",
    "# Simply running PPC will use the updated values and do prediction\n",
    "ppc = pm.sample_ppc(tr1, model=bioassay_model, samples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logitInv= lambda x: np.exp(x)/(1.0+np.exp(x)) #sigmoid --> returns probability\n",
    "logitInv(0.9053831851570006 + 2.9168537609096776 * 0.08986007 + 1.9854704735996134 * 2.57440271)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(max_depth=100,max_features='sqrt', random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.feature_importances_)\n",
    "\n",
    "print( cross_val_score(clf, X_train, y_train, cv=10 ).mean() ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.predict([[2.57440271,  0.08986007]]))\n",
    "clf.predict_proba([[2.57440271,  0.08986007]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression - Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic Regression\n",
    "logit = LogisticRegression(fit_intercept=True)\n",
    "\n",
    "# Fit model. Let X_train = matrix of predictors, y_train = matrix of variable.\n",
    "# NOTE: Do not include a column for the intercept when fitting the model.\n",
    "resLogit = logit.fit(X_train, y_train)\n",
    "print(resLogit.coef_)\n",
    "print(cross_val_score(resLogit, X_train, y_train, cv=10 ).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( cross_val_score(u, X_train, y_train, cv=10 ).mean() ,\n",
    "cross_val_score(clf, X_train, y_train, cv=10 ).mean(),\n",
    "cross_val_score(resLogit, X_train, y_train, cv=10 ).mean() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = u.predict_proba(X_train)\n",
    "preds = probs[:,1]\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_train, preds)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "pforest = clf.predict_proba(X_train)\n",
    "pforest1 = pforest[:,1]\n",
    "fpr1, tpr1, threshold1 = metrics.roc_curve(y_train, pforest1)\n",
    "roc_auc1 = metrics.auc(fpr1, tpr1)\n",
    "\n",
    "plog = logit.predict_proba(X_train)\n",
    "plog1 = plog[:,1]\n",
    "fpr2, tpr2, threshold2 = metrics.roc_curve(y_train, plog1)\n",
    "roc_auc2 = metrics.auc(fpr2, tpr2)\n",
    "\n",
    "# method I: plt\n",
    "#import matplotlib.pyplot as plt\n",
    "pyplot.title('Receiver Operating Characteristic')\n",
    "pyplot.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "pyplot.plot(fpr1, tpr1, 'g', label = 'AUC_f = %0.2f' % roc_auc1)\n",
    "pyplot.plot(fpr2, tpr2, 'y', label = 'AUC_L = %0.2f' % roc_auc2)\n",
    "pyplot.legend(loc = 'lower right')\n",
    "pyplot.plot([0, 1], [0, 1],'r--')\n",
    "pyplot.xlim([0, 1])\n",
    "pyplot.ylim([0, 1])\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate Data (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Toy Data Random Forest data\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=4,\n",
    "                            n_informative=2, n_redundant=0,\n",
    "                            random_state=0, shuffle=False)\n",
    "X_train = scaler.fit(X).transform(X)\n",
    "y_train = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(max_depth=100,max_features='sqrt', random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(clf.feature_importances_)\n",
    "\n",
    "print(cross_val_score(clf, X_train, y_train, cv=10 ),\n",
    "np.abs(cross_val_score(clf, X_train, y_train, cv=10 ).mean()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.predict([[-0.57643759, -0.6723759 , -0.23639363,  0.54680607]]) , \n",
    "      clf.predict_proba([[-0.57643759, -0.6723759 , -0.23639363,  0.54680607]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression - Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logit = LogisticRegression(fit_intercept=True)\n",
    "\n",
    "# Fit model. Let X_train = matrix of predictors, y_train = matrix of variable.\n",
    "# NOTE: Do not include a column for the intercept when fitting the model.\n",
    "resLogit = logit.fit(X_train, y_train)\n",
    "print('Coeff: ',resLogit.coef_)\n",
    "print('Acc: ',cross_val_score(resLogit, X_train, y_train, cv=10 )\n",
    "      , np.abs(cross_val_score(resLogit, X_train, y_train, cv=10 ).mean()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Sklearn vs Statsmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate logistic regression object\n",
    "logit = LogisticRegression(C=1e9,fit_intercept=True)\n",
    "\n",
    "# Fit model. Let X_train = matrix of predictors, y_train = matrix of variable.\n",
    "# NOTE: Do not include a column for the intercept when fitting the model.\n",
    "resLogit = logit.fit(X_train, y_train)\n",
    "#print(resLogit.intercept_,resLogit.coef_)\n",
    "\n",
    "# Calculate matrix of predicted class probabilities. \n",
    "# Check resLogit.classes_ to make sure that sklearn ordered your classes as expected\n",
    "predProbs = np.matrix(resLogit.predict_proba(X_train))\n",
    "\n",
    "# Design matrix -- add column of 1's at the beginning of your X_train matrix\n",
    "X_design = np.column_stack((np.ones(shape = X_train.shape[0]), X_train))\n",
    "#np.ones(shape = X_train.shape[0])\n",
    "#X_design =X_train\n",
    "\n",
    "# Initiate matrix of 0's, fill diagonal with each predicted observation's variance\n",
    "V = np.matrix(np.zeros(shape = (X_design.shape[0], X_design.shape[0])))\n",
    "np.fill_diagonal(V, np.multiply(predProbs[:,0], predProbs[:,1]).A1)\n",
    "\n",
    "# Covariance matrix\n",
    "covLogit = np.linalg.inv(X_design.T * V * X_design)\n",
    "#print(\"Covariance matrix: \", covLogit)\n",
    "\n",
    "# Standard errors\n",
    "print(\"Standard errors: \", np.sqrt(np.diag(covLogit)) )\n",
    "\n",
    "# Wald statistic (coefficient / s.e.) ^ 2\n",
    "logitParams = np.insert(resLogit.coef_, 0, resLogit.intercept_)\n",
    "print(\"Coefficients:    \",logitParams)\n",
    "#print( \"Wald statistics: \", (logitParams / np.sqrt(np.diag(covLogit))) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Logistic Regression - statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm\n",
    " \n",
    "model = sm.Logit(y_train, X_design)\n",
    " \n",
    "result =model.fit() #model.fit(method='bfgs')\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Standard errors: \", np.sqrt(np.diag(covLogit)) )\n",
    "logitParams = np.insert(resLogit.coef_, 0, resLogit.intercept_)\n",
    "print([round(float(c+(1.96*v)),3) for c,v in zip(logitParams,np.sqrt(np.diag(covLogit)))])\n",
    "print([round(float(x),3) for x in logitParams])\n",
    "print([round(float(c-(1.96*v)),3) for c,v in zip(logitParams,np.sqrt(np.diag(covLogit)))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[round(float(c+1.96*v),3) for c,v in zip(logitParams,np.sqrt(np.diag(covLogit)))]\n",
    "[round(float(c-1.96*v),3) for c,v in zip(logitParams,np.sqrt(np.diag(covLogit)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistic Regression Bayesian Inference approach :\n",
    "    - Pymc3 with Theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Use theano shared variable so that we can make predictions for new values later\n",
    "log_dose_shared0 = shared(X_train[:, 0])\n",
    "log_dose_shared1 = shared(X_train[:, 1])\n",
    "log_dose_shared2 = shared(X_train[:, 2])\n",
    "log_dose_shared3 = shared(X_train[:, 3])\n",
    "\n",
    "# Sample size in each group. The sample size has to be a shared variable too\n",
    "# Each row/observation is a group so n = total in group. 1 if only one per group\n",
    "n_shared = shared(np.ones(len(X_train), dtype=int))\n",
    "\n",
    "# Outcomes/Target\n",
    "deaths = y_train\n",
    "\n",
    "\n",
    "# 2 Build Probabilistic Model\n",
    "with Model() as bioassay_model:\n",
    "\n",
    "    # Priors for unknown model parameters. e.g. Logit-linear model parameters\n",
    "    alpha = Normal('alpha', 0, sd=100)\n",
    "    beta0 = Normal('beta0', 0, sd=100)\n",
    "    beta1 = Normal('beta1', 0, sd=100)\n",
    "    beta2 = Normal('beta2', 0, sd=100)\n",
    "    beta3 = Normal('beta3', 0, sd=100)\n",
    "    \n",
    "    # Expected value of outcome. e.g. link function outcome. Calculate probabilities of Y/Target\n",
    "    theta = invlogit(alpha + beta0 * log_dose_shared0 + beta1 * log_dose_shared1\\\n",
    "                     + beta2 * log_dose_shared2 + beta3 * log_dose_shared3 )\n",
    "\n",
    "    # Likelihood (sampling distribution) of observations Data likelihood YTarget\n",
    "    obs_deaths = Binomial('obs_deaths', n=n_shared, p=theta, observed=deaths)\n",
    "\n",
    "    \n",
    "# 3 Finds the local maximum a posteriori point given a model. uses BFGS.\n",
    "from pymc3 import find_MAP\n",
    "# Runs fit to data returns parameters/coefficients\n",
    "map_estimate = find_MAP(model=bioassay_model)\n",
    "print(map_estimate)\n",
    "\n",
    "\n",
    "# 4 Now draw samples from the posterior using the given step methods.\n",
    "with bioassay_model:\n",
    "    \n",
    "    # obtain starting values via MAP\n",
    "    start = find_MAP(model=bioassay_model)\n",
    "    \n",
    "    # instantiate sampler\n",
    "    step = pm.Metropolis()\n",
    "    \n",
    "    # posterior of X's\n",
    "    # draw 1,000 posterior samples of independent variables\n",
    "    bioassay_trace = sample(1000, step=step, start=start)\n",
    "\n",
    "\n",
    "# 5 Generate posterior predictive samples from a model given a trace.\n",
    "from pymc3 import sample_ppc\n",
    "\n",
    "with bioassay_model:\n",
    "    deaths_sim = sample_ppc(bioassay_trace, samples=1000)\n",
    "    \n",
    "# take only last half  of posterior distr. of X's. other half was burn in.\n",
    "tr1 = bioassay_trace[500:]\n",
    "    \n",
    "#PREDICT\n",
    "log_dose_to_predict0 = X_train[:1000,0] #np.random.uniform(-0.8,0.7,size=50)\n",
    "log_dose_to_predict1 = X_train[:1000,1] #np.random.uniform(-0.8,0.7,size=50)\n",
    "log_dose_to_predict2 = X_train[:1000,2] #np.random.uniform(-0.8,0.7,size=50)\n",
    "log_dose_to_predict3 = X_train[:1000,3] #np.random.uniform(-0.8,0.7,size=50)\n",
    "n_predict = n = np.ones(1000, dtype=int)\n",
    "\n",
    "# Changing values here will also change values in the model\n",
    "log_dose_shared0.set_value(log_dose_to_predict0)\n",
    "log_dose_shared1.set_value(log_dose_to_predict1)\n",
    "log_dose_shared2.set_value(log_dose_to_predict2)\n",
    "log_dose_shared3.set_value(log_dose_to_predict3)\n",
    "n_shared.set_value(n_predict)\n",
    "\n",
    "# Simply running PPC will use the updated values and do prediction\n",
    "ppc = pm.sample_ppc(tr1, model=bioassay_model, samples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( 'Accuracy:',(ppc['obs_deaths']==y[:1000]).mean() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[249:251],y_train[249:251]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing values here will also change values in the model\n",
    "log_dose_shared0.set_value(np.array([-0.57643759]))\n",
    "log_dose_shared1.set_value(np.array([-0.6723759]))\n",
    "log_dose_shared2.set_value(np.array([-0.23639363]))\n",
    "log_dose_shared3.set_value(np.array([0.54680607]))\n",
    "n_shared.set_value(np.array([1]))\n",
    "\n",
    "# Simply running PPC will use the updated values and do prediction\n",
    "ppc = pm.sample_ppc(tr1, model=bioassay_model, samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppc['obs_deaths'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing values here will also change values in the model\n",
    "log_dose_shared0.set_value(np.array([-0.98566996]))\n",
    "log_dose_shared1.set_value(np.array([1.12344181]))\n",
    "log_dose_shared2.set_value(np.array([-0.35003196]))\n",
    "log_dose_shared3.set_value(np.array([-1.1158904]))\n",
    "n_shared.set_value(np.array([1]))\n",
    "\n",
    "# Simply running PPC will use the updated values and do prediction\n",
    "ppc = pm.sample_ppc(tr1, model=bioassay_model, samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppc['obs_deaths'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logitInv= lambda x: np.exp(x)/(1.0+np.exp(x)) #sigmoid --> returns probability\n",
    "logitInv(map_estimate['alpha']+map_estimate['beta0']*X_train[249:251][1][0]+\\\n",
    "         map_estimate['beta1']*X_train[249:251][1][1]\\\n",
    "+map_estimate['beta2']*X_train[249:251][1][2]+map_estimate['beta3']*X_train[249:251][1][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import random, math\n",
    "\n",
    "# def k_fold(data, myseed=11109, k=10):\n",
    "#     # Load data\n",
    "#     #data = open(myfile).readlines()\n",
    "\n",
    "#     # Shuffle input\n",
    "#     random.seed=myseed\n",
    "#     random.shuffle(data)\n",
    "\n",
    "#     # Compute partition size given input k\n",
    "#     len_part=int(math.ceil(len(data)/float(k)))\n",
    "\n",
    "#     # Create one partition per fold\n",
    "#     train={}\n",
    "#     test={}\n",
    "#     for ii in range(k):\n",
    "#         test[ii]  = data[ii*len_part:ii*len_part+len_part]\n",
    "#         train[ii] = [jj for jj in data if jj not in test[ii]]\n",
    "\n",
    "#     return train, test \n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# plt.errorbar(x=log_dose_to_predict0[:50], y=np.asarray(ppc['obs_deaths']).mean(axis=0)[:50], yerr=np.asarray(ppc['obs_deaths']).std(axis=0)[:50], linestyle='', marker='o')\n",
    "# plt.plot(X_train[:50, 1], deaths[:50], 'o')\n",
    "# plt.xlabel('log_dose',size=15)s\n",
    "# plt.ylabel('number of rats with tumors',size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rf = RandomForestRegressor(n_estimators = 100, max_features='sqrt')\n",
    "# rf.fit(X, y)\n",
    "# # feature importances\n",
    "# # the higher, the more important the feature\n",
    "# d = {'importance': rf.feature_importances_}\n",
    "# pd.DataFrame(d, index=X.columns).sort('importance')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
