{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.stats as st \n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from patsy import dmatrices\n",
    "from sklearn.ensemble.forest import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "from pymc3 import Normal, Binomial, sample, Model # Import relevant distributions\n",
    "from pymc3.math import invlogit\n",
    "# Use a theano shared variable to be able to exchange the data the model runs on\n",
    "from theano import shared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate Data (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TOY DATA LOGISTIC Regression\n",
    "#> set.seed(666)\n",
    "np.random.seed(seed=233423)\n",
    "x1 = st.norm.rvs(size=10000)           # some continuous variables \n",
    "x2 = st.norm.rvs(size=10000)  \n",
    "z = 1 + 2 *x1 +3 *x2        # linear combination with a bias\n",
    "pr = 1/(1 +np.exp(-z))         # pass through an inv-logit function\n",
    "y = st.binom.rvs(n=1,p=pr, size=10000) #rbinom(1000,1,pr) # bernoulli response variable\n",
    " \n",
    "X=np.column_stack([x1,x2])\n",
    "# standardize the features since regularization requires all features to be on same scale\n",
    "scaler = StandardScaler(copy=True)\n",
    "# we have created a standardization based on the training data\n",
    "X_train = scaler.fit(X).transform(X)\n",
    "y_train = y\n",
    "\n",
    "#now feed it to glm:\n",
    "#df = data.frame(y=y,x1=x1,x2=x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistic Regression Bayesian Inference approach :\n",
    "    - Pymc3 with Theano\n",
    "\n",
    "    Steps:\n",
    "        1. Prepare Data\n",
    "        2. Build Probabilistic Model\n",
    "        3. Condition Model on Data & Find the local maximum a posteriori point given a model (MAP)\n",
    "        4. Sample posterior distribution using MAP as starting points for Indepedent Variables(X's)\n",
    "        5. Generate posterior predictive samples from model given a Samples of posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "logp = -3,120.2, ||grad|| = 1.7939: 100%|██████████████████████████████████████████████| 14/14 [00:01<00:00,  9.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': array(0.9053831851570006), 'beta': array(2.9168537609096776), 'beta0': array(1.9854704735996134)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "logp = -3,120.2, ||grad|| = 1.7939: 100%|██████████████████████████████████████████████| 14/14 [00:01<00:00, 12.06it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1500/1500 [06:31<00:00,  3.83it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:02<00:00, 494.21it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 500/500 [00:00<00:00, 1809.41it/s]\n"
     ]
    }
   ],
   "source": [
    "# 1 Use theano shared variable so that we can make predictions for new values later\n",
    "log_dose_shared0 = shared(X_train[:, 0])\n",
    "log_dose_shared = shared(X_train[:, 1])\n",
    "\n",
    "# Sample size in each group. The sample size has to be a shared variable too\n",
    "# Each row/observation is a group so n = total in group. 1 if only one per group\n",
    "n_shared = shared(np.ones(len(X_train), dtype=int))\n",
    "\n",
    "# Outcomes/Target\n",
    "deaths = y_train\n",
    "\n",
    "\n",
    "# 2 Build Probabilistic Model\n",
    "with Model() as bioassay_model:\n",
    "\n",
    "    # Priors for unknown model parameters. e.g. Logit-linear model parameters\n",
    "    alpha = Normal('alpha', 0, sd=100)\n",
    "    beta = Normal('beta', 0, sd=100)\n",
    "    beta0 = Normal('beta0', 0, sd=100)\n",
    "\n",
    "    # Expected value of outcome. e.g. link function outcome. Calculate probabilities of Y/Target\n",
    "    theta = invlogit(alpha + beta * log_dose_shared + beta0 * log_dose_shared0 )\n",
    "\n",
    "    # Likelihood (sampling distribution) of observations Data likelihood YTarget\n",
    "    obs_deaths = Binomial('obs_deaths', n=n_shared, p=theta, observed=deaths)\n",
    "\n",
    "    \n",
    "# 3 Finds the local maximum a posteriori point given a model. uses BFGS.\n",
    "from pymc3 import find_MAP\n",
    "# Runs fit to data returns parameters/coefficients\n",
    "map_estimate = find_MAP(model=bioassay_model)\n",
    "print(map_estimate)\n",
    "\n",
    "\n",
    "# 4 Now draw samples from the posterior using the given step methods.\n",
    "with bioassay_model:\n",
    "    \n",
    "    # obtain starting values via MAP\n",
    "    start = find_MAP(model=bioassay_model)\n",
    "    \n",
    "    # instantiate sampler\n",
    "    step = pm.Metropolis()\n",
    "    \n",
    "    # posterior of X's\n",
    "    # draw 1,000 posterior samples of independent variables\n",
    "    bioassay_trace = sample(1000, step=step, start=start)\n",
    "\n",
    "\n",
    "# 5 Generate posterior predictive samples from a model given a trace.\n",
    "from pymc3 import sample_ppc\n",
    "\n",
    "with bioassay_model:\n",
    "    deaths_sim = sample_ppc(bioassay_trace, samples=1000)\n",
    "    \n",
    "# take only last half  of posterior distr. of X's. other half was burn in.\n",
    "tr1 = bioassay_trace[500:]\n",
    "    \n",
    "#PREDICT\n",
    "log_dose_to_predict0 = X_train[:1000,0] #np.random.uniform(-0.8,0.7,size=50)\n",
    "log_dose_to_predict = X_train[:1000,1] #np.random.uniform(-0.8,0.7,size=50)\n",
    "n_predict = n = np.ones(1000, dtype=int)\n",
    "\n",
    "# Changing values here will also change values in the model\n",
    "log_dose_shared0.set_value(log_dose_to_predict0)\n",
    "log_dose_shared.set_value(log_dose_to_predict)\n",
    "n_shared.set_value(n_predict)\n",
    "\n",
    "# Simply running PPC will use the updated values and do prediction\n",
    "ppc = pm.sample_ppc(tr1, model=bioassay_model, samples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.805806\n"
     ]
    }
   ],
   "source": [
    "print( 'Accuracy:',(ppc['obs_deaths']==y[:1000]).mean() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.71438705,  1.04372761],\n",
       "       [ 0.44338059,  2.91463364],\n",
       "       [ 0.20930136,  1.06472176],\n",
       "       [ 2.57440271,  0.08986007],\n",
       "       [ 0.93622719, -1.2161206 ],\n",
       "       [-0.57650503, -1.95645393],\n",
       "       [-0.71714114, -0.10367281],\n",
       "       [-0.82816794,  0.65578596]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 500/500 [00:00<00:00, 2292.71it/s]\n"
     ]
    }
   ],
   "source": [
    "# X0 , X  columns and  theta formula swapped\n",
    "# [ 2.57440271,  0.08986007], #Target = 1\n",
    "# [ 0.93622719, -1.2161206 ] #Target = 0\n",
    "# Changing values here will also change values in the model\n",
    "log_dose_shared0.set_value(np.array([ 0.93622719]))\n",
    "log_dose_shared.set_value(np.array([-1.2161206]))\n",
    "n_shared.set_value(np.array([1]))\n",
    "\n",
    "# Simply running PPC will use the updated values and do prediction\n",
    "ppc = pm.sample_ppc(tr1, model=bioassay_model, samples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99812803417373919"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logitInv= lambda x: np.exp(x)/(1.0+np.exp(x)) #sigmoid --> returns probability\n",
    "logitInv(0.9053831851570006 + 2.9168537609096776 * 0.08986007 + 1.9854704735996134 * 2.57440271)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.40173059  0.59826941]\n",
      "[ 0.84115884  0.82117882  0.836       0.829       0.814       0.834       0.821\n",
      "  0.832       0.82182182  0.82082082] 0.827098030498\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(max_depth=100,max_features='sqrt', random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(clf.feature_importances_)\n",
    "\n",
    "#print(clf.predict([[0, 0, 0, 0]]))\n",
    "# not such a great result with our naive random forest...\n",
    "# we take np.abs because scikit learn returns a negative number for mean_absolute_error by default\n",
    "\n",
    "print(cross_val_score(clf, X_train, y_train, cv=10 ),\n",
    "np.abs(cross_val_score(clf, X_train, y_train, cv=10 ).mean()) )\n",
    "#cross_val_score(model, df, y, cv=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.1,  0.9]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(clf.predict([[2.57440271,  0.08986007]]))\n",
    "clf.predict_proba([[2.57440271,  0.08986007]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.9741301   2.90014277]]\n",
      "[ 0.85514486  0.83716284  0.863       0.845       0.865       0.88        0.86\n",
      "  0.859       0.84784785  0.85985986] 0.857201540002\n"
     ]
    }
   ],
   "source": [
    "#logistic Regression\n",
    "logit = LogisticRegression(fit_intercept=True)\n",
    "\n",
    "# Fit model. Let X_train = matrix of predictors, y_train = matrix of variable.\n",
    "# NOTE: Do not include a column for the intercept when fitting the model.\n",
    "resLogit = logit.fit(X_train, y_train)\n",
    "print(resLogit.coef_)\n",
    "print(cross_val_score(resLogit, X_train, y_train, cv=10 ),\n",
    "      np.abs(cross_val_score(resLogit, X_train, y_train, cv=10 ).mean()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate Data (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Toy Data Random Forest data\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=4,\n",
    "                            n_informative=2, n_redundant=0,\n",
    "                            random_state=0, shuffle=False)\n",
    "X_train = scaler.fit(X).transform(X)\n",
    "y_train = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.09424372  0.83597175  0.03343739  0.03634714]\n",
      "[ 0.92079208  0.97029703  0.99009901  0.97029703  0.95        0.95\n",
      "  0.92929293  0.92929293  0.93939394  0.8989899 ] 0.944845484548\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(max_depth=100,max_features='sqrt', random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(clf.feature_importances_)\n",
    "\n",
    "print(cross_val_score(clf, X_train, y_train, cv=10 ),\n",
    "np.abs(cross_val_score(clf, X_train, y_train, cv=10 ).mean()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] [[ 1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(clf.predict([[-0.57643759, -0.6723759 , -0.23639363,  0.54680607]]) , \n",
    "      clf.predict_proba([[-0.57643759, -0.6723759 , -0.23639363,  0.54680607]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistic Regression - Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeff:  [[-0.48062427  4.20389638  0.09518497 -0.25831008]]\n",
      "Acc:  [ 0.93069307  0.94059406  0.95049505  0.97029703  0.95        0.96\n",
      "  0.95959596  0.92929293  0.93939394  0.92929293] 0.94596549655\n"
     ]
    }
   ],
   "source": [
    "logit = LogisticRegression(fit_intercept=True)\n",
    "\n",
    "# Fit model. Let X_train = matrix of predictors, y_train = matrix of variable.\n",
    "# NOTE: Do not include a column for the intercept when fitting the model.\n",
    "resLogit = logit.fit(X_train, y_train)\n",
    "print('Coeff: ',resLogit.coef_)\n",
    "print('Acc: ',cross_val_score(resLogit, X_train, y_train, cv=10 )\n",
    "      , np.abs(cross_val_score(resLogit, X_train, y_train, cv=10 ).mean()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Sklearn vs Statsmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard errors:  [ 0.14894159  0.15649742  0.29668446  0.14120487  0.14041639]\n",
      "Coefficients:     [ 0.32993529 -0.57036084  4.54675544  0.1001741  -0.28019433]\n"
     ]
    }
   ],
   "source": [
    "# Initiate logistic regression object\n",
    "logit = LogisticRegression(C=1e9,fit_intercept=True)\n",
    "\n",
    "# Fit model. Let X_train = matrix of predictors, y_train = matrix of variable.\n",
    "# NOTE: Do not include a column for the intercept when fitting the model.\n",
    "resLogit = logit.fit(X_train, y_train)\n",
    "#print(resLogit.intercept_,resLogit.coef_)\n",
    "\n",
    "# Calculate matrix of predicted class probabilities. \n",
    "# Check resLogit.classes_ to make sure that sklearn ordered your classes as expected\n",
    "predProbs = np.matrix(resLogit.predict_proba(X_train))\n",
    "\n",
    "# Design matrix -- add column of 1's at the beginning of your X_train matrix\n",
    "X_design = np.column_stack((np.ones(shape = X_train.shape[0]), X_train))\n",
    "#np.ones(shape = X_train.shape[0])\n",
    "#X_design =X_train\n",
    "\n",
    "# Initiate matrix of 0's, fill diagonal with each predicted observation's variance\n",
    "V = np.matrix(np.zeros(shape = (X_design.shape[0], X_design.shape[0])))\n",
    "np.fill_diagonal(V, np.multiply(predProbs[:,0], predProbs[:,1]).A1)\n",
    "\n",
    "# Covariance matrix\n",
    "covLogit = np.linalg.inv(X_design.T * V * X_design)\n",
    "#print(\"Covariance matrix: \", covLogit)\n",
    "\n",
    "# Standard errors\n",
    "print(\"Standard errors: \", np.sqrt(np.diag(covLogit)) )\n",
    "\n",
    "# Wald statistic (coefficient / s.e.) ^ 2\n",
    "logitParams = np.insert(resLogit.coef_, 0, resLogit.intercept_)\n",
    "print(\"Coefficients:    \",logitParams)\n",
    "#print( \"Wald statistics: \", (logitParams / np.sqrt(np.diag(covLogit))) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistic Regression - statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.184428\n",
      "         Iterations 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>y</td>        <th>  No. Observations:  </th>   <td>  1000</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>   <td>   995</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>   <td>     4</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Sun, 15 Oct 2017</td> <th>  Pseudo R-squ.:     </th>   <td>0.7339</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>20:46:42</td>     <th>  Log-Likelihood:    </th>  <td> -184.43</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th>  <td> -693.12</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th> <td>6.125e-219</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    0.3300</td> <td>    0.149</td> <td>    2.215</td> <td> 0.027</td> <td>    0.038</td> <td>    0.622</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>   -0.5704</td> <td>    0.156</td> <td>   -3.645</td> <td> 0.000</td> <td>   -0.877</td> <td>   -0.264</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    4.5468</td> <td>    0.297</td> <td>   15.325</td> <td> 0.000</td> <td>    3.965</td> <td>    5.128</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>    0.1002</td> <td>    0.141</td> <td>    0.710</td> <td> 0.478</td> <td>   -0.177</td> <td>    0.377</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>   -0.2802</td> <td>    0.140</td> <td>   -1.995</td> <td> 0.046</td> <td>   -0.555</td> <td>   -0.005</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   No. Observations:                 1000\n",
       "Model:                          Logit   Df Residuals:                      995\n",
       "Method:                           MLE   Df Model:                            4\n",
       "Date:                Sun, 15 Oct 2017   Pseudo R-squ.:                  0.7339\n",
       "Time:                        20:46:42   Log-Likelihood:                -184.43\n",
       "converged:                       True   LL-Null:                       -693.12\n",
       "                                        LLR p-value:                6.125e-219\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          0.3300      0.149      2.215      0.027       0.038       0.622\n",
       "x1            -0.5704      0.156     -3.645      0.000      -0.877      -0.264\n",
       "x2             4.5468      0.297     15.325      0.000       3.965       5.128\n",
       "x3             0.1002      0.141      0.710      0.478      -0.177       0.377\n",
       "x4            -0.2802      0.140     -1.995      0.046      -0.555      -0.005\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.formula.api as sm\n",
    " \n",
    "model = sm.Logit(y_train, X_design)\n",
    " \n",
    "result =model.fit() #model.fit(method='bfgs')\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard errors:  [ 0.14894159  0.15649742  0.29668446  0.14120487  0.14041639]\n",
      "[0.622, -0.264, 5.128, 0.377, -0.005]\n",
      "[0.33, -0.57, 4.547, 0.1, -0.28]\n",
      "[0.038, -0.877, 3.965, -0.177, -0.555]\n"
     ]
    }
   ],
   "source": [
    "print(\"Standard errors: \", np.sqrt(np.diag(covLogit)) )\n",
    "logitParams = np.insert(resLogit.coef_, 0, resLogit.intercept_)\n",
    "print([round(float(c+(1.96*v)),3) for c,v in zip(logitParams,np.sqrt(np.diag(covLogit)))])\n",
    "print([round(float(x),3) for x in logitParams])\n",
    "print([round(float(c-(1.96*v)),3) for c,v in zip(logitParams,np.sqrt(np.diag(covLogit)))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.038, -0.877, 3.965, -0.177, -0.555]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[round(float(c+1.96*v),3) for c,v in zip(logitParams,np.sqrt(np.diag(covLogit)))]\n",
    "[round(float(c-1.96*v),3) for c,v in zip(logitParams,np.sqrt(np.diag(covLogit)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.27953753, -1.07614812,  0.34409454, -0.61389564],\n",
       "       [-2.26261269, -0.89834916,  0.78188638,  0.43062548],\n",
       "       [-0.47128857, -1.13624301, -3.07536742,  0.65616331],\n",
       "       ..., \n",
       "       [ 0.66923396,  0.95782361,  0.94200115, -2.29563862],\n",
       "       [ 0.05359401,  1.25619157, -0.38007636, -0.68146826],\n",
       "       [ 0.76300106,  0.00566496,  0.22060183, -1.97436012]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistic Regression Bayesian Inference approach :\n",
    "    - Pymc3 with Theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "logp = -212.05, ||grad|| = 0.02526: 100%|█████████████████████████████████████████████| 15/15 [00:00<00:00, 161.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': array(0.3299516403912339), 'beta2': array(0.10019578934956035), 'beta0': array(-0.5703753341549249), 'beta1': array(4.546782324385807), 'beta3': array(-0.280172519081352)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "logp = -212.05, ||grad|| = 0.02526: 100%|█████████████████████████████████████████████| 15/15 [00:00<00:00, 141.84it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1500/1500 [00:45<00:00, 32.84it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 783.68it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 500/500 [00:00<00:00, 2617.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# 1 Use theano shared variable so that we can make predictions for new values later\n",
    "log_dose_shared0 = shared(X_train[:, 0])\n",
    "log_dose_shared1 = shared(X_train[:, 1])\n",
    "log_dose_shared2 = shared(X_train[:, 2])\n",
    "log_dose_shared3 = shared(X_train[:, 3])\n",
    "\n",
    "# Sample size in each group. The sample size has to be a shared variable too\n",
    "# Each row/observation is a group so n = total in group. 1 if only one per group\n",
    "n_shared = shared(np.ones(len(X_train), dtype=int))\n",
    "\n",
    "# Outcomes/Target\n",
    "deaths = y_train\n",
    "\n",
    "\n",
    "# 2 Build Probabilistic Model\n",
    "with Model() as bioassay_model:\n",
    "\n",
    "    # Priors for unknown model parameters. e.g. Logit-linear model parameters\n",
    "    alpha = Normal('alpha', 0, sd=100)\n",
    "    beta0 = Normal('beta0', 0, sd=100)\n",
    "    beta1 = Normal('beta1', 0, sd=100)\n",
    "    beta2 = Normal('beta2', 0, sd=100)\n",
    "    beta3 = Normal('beta3', 0, sd=100)\n",
    "    \n",
    "    # Expected value of outcome. e.g. link function outcome. Calculate probabilities of Y/Target\n",
    "    theta = invlogit(alpha + beta0 * log_dose_shared0 + beta1 * log_dose_shared1\\\n",
    "                     + beta2 * log_dose_shared2 + beta3 * log_dose_shared3 )\n",
    "\n",
    "    # Likelihood (sampling distribution) of observations Data likelihood YTarget\n",
    "    obs_deaths = Binomial('obs_deaths', n=n_shared, p=theta, observed=deaths)\n",
    "\n",
    "    \n",
    "# 3 Finds the local maximum a posteriori point given a model. uses BFGS.\n",
    "from pymc3 import find_MAP\n",
    "# Runs fit to data returns parameters/coefficients\n",
    "map_estimate = find_MAP(model=bioassay_model)\n",
    "print(map_estimate)\n",
    "\n",
    "\n",
    "# 4 Now draw samples from the posterior using the given step methods.\n",
    "with bioassay_model:\n",
    "    \n",
    "    # obtain starting values via MAP\n",
    "    start = find_MAP(model=bioassay_model)\n",
    "    \n",
    "    # instantiate sampler\n",
    "    step = pm.Metropolis()\n",
    "    \n",
    "    # posterior of X's\n",
    "    # draw 1,000 posterior samples of independent variables\n",
    "    bioassay_trace = sample(1000, step=step, start=start)\n",
    "\n",
    "\n",
    "# 5 Generate posterior predictive samples from a model given a trace.\n",
    "from pymc3 import sample_ppc\n",
    "\n",
    "with bioassay_model:\n",
    "    deaths_sim = sample_ppc(bioassay_trace, samples=1000)\n",
    "    \n",
    "# take only last half  of posterior distr. of X's. other half was burn in.\n",
    "tr1 = bioassay_trace[500:]\n",
    "    \n",
    "#PREDICT\n",
    "log_dose_to_predict0 = X_train[:1000,0] #np.random.uniform(-0.8,0.7,size=50)\n",
    "log_dose_to_predict1 = X_train[:1000,1] #np.random.uniform(-0.8,0.7,size=50)\n",
    "log_dose_to_predict2 = X_train[:1000,2] #np.random.uniform(-0.8,0.7,size=50)\n",
    "log_dose_to_predict3 = X_train[:1000,3] #np.random.uniform(-0.8,0.7,size=50)\n",
    "n_predict = n = np.ones(1000, dtype=int)\n",
    "\n",
    "# Changing values here will also change values in the model\n",
    "log_dose_shared0.set_value(log_dose_to_predict0)\n",
    "log_dose_shared1.set_value(log_dose_to_predict1)\n",
    "log_dose_shared2.set_value(log_dose_to_predict2)\n",
    "log_dose_shared3.set_value(log_dose_to_predict3)\n",
    "n_shared.set_value(n_predict)\n",
    "\n",
    "# Simply running PPC will use the updated values and do prediction\n",
    "ppc = pm.sample_ppc(tr1, model=bioassay_model, samples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.902246\n"
     ]
    }
   ],
   "source": [
    "print( 'Accuracy:',(ppc['obs_deaths']==y[:1000]).mean() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.57643759, -0.6723759 , -0.23639363,  0.54680607],\n",
       "        [-0.98566996,  1.12344181, -0.35003196, -1.1158904 ]]), array([0, 1]))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[249:251],y_train[249:251]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 3965.11it/s]\n"
     ]
    }
   ],
   "source": [
    "# Changing values here will also change values in the model\n",
    "log_dose_shared0.set_value(np.array([-0.57643759]))\n",
    "log_dose_shared1.set_value(np.array([-0.6723759]))\n",
    "log_dose_shared2.set_value(np.array([-0.23639363]))\n",
    "log_dose_shared3.set_value(np.array([0.54680607]))\n",
    "n_shared.set_value(np.array([1]))\n",
    "\n",
    "# Simply running PPC will use the updated values and do prediction\n",
    "ppc = pm.sample_ppc(tr1, model=bioassay_model, samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.073999999999999996"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppc['obs_deaths'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 3934.30it/s]\n"
     ]
    }
   ],
   "source": [
    "# Changing values here will also change values in the model\n",
    "log_dose_shared0.set_value(np.array([-0.98566996]))\n",
    "log_dose_shared1.set_value(np.array([1.12344181]))\n",
    "log_dose_shared2.set_value(np.array([-0.35003196]))\n",
    "log_dose_shared3.set_value(np.array([-1.1158904]))\n",
    "n_shared.set_value(np.array([1]))\n",
    "\n",
    "# Simply running PPC will use the updated values and do prediction\n",
    "ppc = pm.sample_ppc(tr1, model=bioassay_model, samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppc['obs_deaths'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': array(0.3299516403912339),\n",
       " 'beta0': array(-0.5703753341549249),\n",
       " 'beta1': array(4.546782324385807),\n",
       " 'beta2': array(0.10019578934956035),\n",
       " 'beta3': array(-0.280172519081352)}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logitInv= lambda x: np.exp(x)/(1.0+np.exp(x)) #sigmoid --> returns probability\n",
    "logitInv(map_estimate['alpha']+map_estimate['beta0']*X_train[249:251][1][0]+\\\n",
    "         map_estimate['beta1']*X_train[249:251][1][1]\\\n",
    "+map_estimate['beta2']*X_train[249:251][1][2]+map_estimate['beta3']*X_train[249:251][1][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import random, math\n",
    "\n",
    "# def k_fold(data, myseed=11109, k=10):\n",
    "#     # Load data\n",
    "#     #data = open(myfile).readlines()\n",
    "\n",
    "#     # Shuffle input\n",
    "#     random.seed=myseed\n",
    "#     random.shuffle(data)\n",
    "\n",
    "#     # Compute partition size given input k\n",
    "#     len_part=int(math.ceil(len(data)/float(k)))\n",
    "\n",
    "#     # Create one partition per fold\n",
    "#     train={}\n",
    "#     test={}\n",
    "#     for ii in range(k):\n",
    "#         test[ii]  = data[ii*len_part:ii*len_part+len_part]\n",
    "#         train[ii] = [jj for jj in data if jj not in test[ii]]\n",
    "\n",
    "#     return train, test \n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# plt.errorbar(x=log_dose_to_predict0[:50], y=np.asarray(ppc['obs_deaths']).mean(axis=0)[:50], yerr=np.asarray(ppc['obs_deaths']).std(axis=0)[:50], linestyle='', marker='o')\n",
    "# plt.plot(X_train[:50, 1], deaths[:50], 'o')\n",
    "# plt.xlabel('log_dose',size=15)s\n",
    "# plt.ylabel('number of rats with tumors',size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rf = RandomForestRegressor(n_estimators = 100, max_features='sqrt')\n",
    "# rf.fit(X, y)\n",
    "# # feature importances\n",
    "# # the higher, the more important the feature\n",
    "# d = {'importance': rf.feature_importances_}\n",
    "# pd.DataFrame(d, index=X.columns).sort('importance')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
